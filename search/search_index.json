{"docs":[{"location":"/paradox.json","text":"","title":""},{"location":"/static/cuya.html","text":"","title":""},{"location":"/index.html","text":"","title":"RasterFrames"},{"location":"/index.html#rasterframes","text":"RasterFrames® brings together Earth-observation (EO) data access, cloud computing, and DataFrame-based data science. The recent explosion of EO data from public and private satellite operators presents both a huge opportunity and a huge challenge to the data analysis community. It is Big Data in the truest sense, and its footprint is rapidly getting bigger.\nRasterFrames provides a DataFrame-centric view over arbitrary raster data, enabling spatiotemporal queries, map algebra raster operations, and compatibility with the ecosystem of Spark ML algorithms. By using DataFrames as the core cognitive and compute data model, it is able to deliver these features in a form that is both accessible to general analysts and scalable along with the rapidly growing data footprint.\nTo learn more, please see the Getting Started section of this manual.\nThe source code can be found on GitHub at locationtech/rasterframes.\nRasterFrames is released under the Apache 2.0 License.","title":"RasterFrames"},{"location":"/index.html#related-links","text":"Gitter Channel Scala API Documentation GitHub Repository Astraea, Inc., the company behind RasterFrames","title":"Related Links"},{"location":"/index.html#detailed-contents","text":"Overview Context Benefit Architecture Getting Started pip install pyrasterframes Next Steps Other Options Using Jupyter Notebook Using pyspark shell Installing GDAL Installing on MacOS Installing on Linux Testing For GDAL Concepts Raster Cell Cell Type NoData Scene Band Coordinate Reference System (CRS) Extent Tile Raster Data I/O Raster Catalogs Creating a Catalog One-D Two-D Using External Catalogs Using Built-in Catalogs MODIS Landsat 8 Reading Raster Data Single Raster URI Formats Raster Catalogs Lazy Raster Reading Multiband Rasters GeoTrellis GeoTrellis Catalogs GeoTrellis Layers Writing Raster Data Tile Samples DataFrame Samples GeoTIFFs GeoTrellis Layers Parquet Vector Data GeoJSON DataSource GeoPandas and RasterFrames Shapely Geometry Support GeoMesa Functions and Spatial Relations Raster Processing Local Map Algebra Computing NDVI “NoData” Handling What is NoData? Cell Types Understanding Cell Types and NoData Masking NoData and Local Arithmetic Changing a Tile’s NoData Values Combining Tiles with Different Data Types NoData Values in Aggregation Aggregation Tile Mean Example Cell Counts Example Statistical Summaries Histogram Time Series Analysis Plan Vector Data Catalog Read Vector and Raster Data Interaction Create Time Series Machine Learning Unsupervised Machine Learning Imports and Data Preparation Create ML Pipeline Fit the Model and Score Visualize Prediction Supervised Machine Learning Create and Read Raster Catalog Data Prep Masking Poor Quality Cells Create ML Pipeline Train the Model Model Evaluation Visualize Prediction NumPy and Pandas Performance Considerations The Tile Class DataFrame toPandas User Defined Functions Creating a Spark DataFrame API Languages Python Step 1: Load the catalog Step 2: Down-select data by month Step 3: Read tiles Step 4: Compute aggregates SQL Step 1: Load the catalog Step 2: Down-select data by month Step 3: Read tiles Step 4: Compute aggregates Scala Step 1: Load the catalog Step 2: Down-select data by month Step 3: Read tiles Step 4: Compute aggregates Function Reference List of Available SQL and Python Functions Vector Operations st_reproject st_extent st_geometry Tile Metadata and Mutation rf_dimensions rf_cell_type rf_tile rf_extent rf_crs rf_mk_crs rf_convert_cell_type rf_interpret_cell_type_as rf_resample Tile Creation rf_make_zeros_tile rf_make_ones_tile rf_make_constant_tile rf_rasterize rf_array_to_tile rf_assemble_tile Masking and NoData rf_mask rf_inverse_mask rf_mask_by_value rf_is_no_data_tile rf_local_no_data rf_local_data rf_local_data rf_with_no_data Local Map Algebra rf_local_add rf_local_subtract rf_local_multiply rf_local_divide rf_normalized_difference rf_local_less rf_local_less_equal rf_local_greater rf_local_greater_equal rf_local_equal rf_local_unequal rf_round rf_abs rf_exp rf_exp10 rf_exp2 rf_expm1 rf_log rf_log10 rf_log2 rf_log1p Tile Statistics rf_tile_sum rf_tile_mean rf_tile_min rf_tile_max rf_no_data_cells rf_data_cells rf_exists rf_for_all rf_tile_stats rf_tile_histogram Aggregate Tile Statistics rf_agg_mean rf_agg_data_cells rf_agg_no_data_cells rf_agg_stats rf_agg_approx_histogram Tile Local Aggregate Statistics rf_agg_local_max rf_agg_local_min rf_agg_local_mean rf_agg_local_data_cells rf_agg_local_no_data_cells rf_agg_local_stats Converting Tiles rf_explode_tiles rf_explode_tiles_sample rf_tile_to_array_int rf_tile_to_array_double rf_render_ascii rf_render_matrix rf_rgb_composite rf_render_png Release Notes 0.8.x 0.8.1 0.8.0 Known issues 0.7.x 0.7.1 0.7.0 0.6.x 0.6.1 0.6.0 Known Issues 0.5.x 0.5.12 0.5.11 0.5.10 0.5.9 0.5.8 0.5.7 0.5.6 0.5.5","title":"Detailed Contents"},{"location":"/description.html","text":"","title":"Overview"},{"location":"/description.html#overview","text":"RasterFrames® provides a DataFrame-centric view over arbitrary Earth-observation (EO) data, enabling spatiotemporal queries, map algebra raster operations, and compatibility with the ecosystem of Apache Spark ML algorithms. It provides APIs in Python, SQL, and Scala, and can scale from a laptop computer to a large distributed cluster, enabling global analysis with satellite imagery in a wholly new, flexible, and convenient way.","title":"Overview"},{"location":"/description.html#context","text":"We have a millennia-long history of organizing information in tabular form. Typically, rows represent independent events or observations, and columns represent attributes and measurements from the observations. The forms have evolved, from hand-written agricultural records and transaction ledgers, to the advent of spreadsheets on the personal computer, and on to the creation of the DataFrame data structure as found in R Data Frames and Python Pandas. The table-oriented data structure remains a common and critical component of organizing data across industries, and—most importantly—it is the mental model employed by data scientists across diverse forms of modeling and analysis.\nThe evolution of the DataFrame form has continued with Spark SQL, which brings DataFrames to the big data distributed compute space. Through several novel innovations, Spark SQL enables data scientists to work with DataFrames too large for the memory of a single computer. As suggested by the name, these DataFrames are manipulatable via standard SQL, as well as the more general-purpose programming languages Python, R, Java, and Scala.\nRasterFrames, an incubating Eclipse Foundation LocationTech project, brings together EO data access, cloud computing, and DataFrame-based data science. The recent explosion of EO data from public and private satellite operators presents both a huge opportunity and a huge challenge to the data analysis community. It is Big Data in the truest sense, and its footprint is rapidly getting bigger. According to a World Bank document on assets for post-disaster situation awareness[^1]:\nOf the 1,738 operational satellites currently orbiting the earth (as of 9/[20]17), 596 are earth observation satellites and 477 of these are non-military assets (i.e. available to civil society including commercial entities and governments for earth observation, according to the Union of Concerned Scientists). This number is expected to increase significantly over the next ten years. The 200 or so planned remote sensing satellites have a value of over 27 billion USD (Forecast International). This estimate does not include the burgeoning fleets of smallsats as well as micro, nano and even smaller satellites… All this enthusiasm has, not unexpectedly, led to a veritable fire-hose of remotely sensed data which is becoming difficult to navigate even for seasoned experts.","title":"Context"},{"location":"/description.html#benefit","text":"By using DataFrames as the core cognitive and compute data model for processing EO data, RasterFrames is able to deliver sophisticated computational and algorithmic capabilities in a tabular form that is familiar and accessible to the general computing public. Because it is built on Apache Spark, solutions prototyped on a laptop can be easily scaled to run on cluster and cloud compute resources. Apache Spark also provides integration between its DataFrame libraries and machine learning, with which RasterFrames is fully compatible.","title":"Benefit"},{"location":"/description.html#architecture","text":"RasterFrames builds upon several other LocationTech projects: GeoTrellis, GeoMesa, JTS, and SFCurve.\nRasterFrames introduces georectified raster imagery to Spark SQL. It quantizes scenes into chunks called tiles. Each tile contains a 2-D matrix of cell or pixel values along with information on how to numerically interpret those cells.\nAs shown in the figure below, a “RasterFrame” is a Spark DataFrame with one or more columns of type tile. A tile column typically represents a single frequency band of sensor data, such as “blue” or “near infrared”, but can also be quality assurance information, land classification assignments, or any other raster spatial data. Along with tile columns there is typically an extent specifying the geographic location of the data, the map projection of that geometry (crs), and a timestamp column representing the acquisition time. These columns can all be used in the WHERE clause when filtering.\ntimestamp crs extent tile 2019-02-28 [+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs ] [-6834789.194217826, 163086.0762178264, -6716181.138786679, 281694.1316489731] 2019-02-28 [+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs ] [-7427829.47137356, 281694.13164897315, -7309221.415942413, 400302.18708011985] 2019-02-28 [+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs ] [-7309221.415942414, 163086.0762178264, -7190613.360511267, 281694.1316489731] 2019-02-28 [+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs ] [-7665045.582235853, 637518.2979424132, -7546437.526804706, 756126.3533735599]\nRasterFrames also includes support for working with vector data, such as GeoJSON. RasterFrames vector data operations let you filter with geospatial relationships like contains or intersects, mask cells, convert vectors to rasters, and more.\nRaster data can be read from a number of sources. Through the flexible Spark SQL DataSource API, RasterFrames can be constructed from collections of imagery (including Cloud Optimized GeoTIFFs or COGS), GeoTrellis Layers, and from catalogs of large datasets like Landsat 8 and MODIS data sets on the AWS Public Data Set (PDS).\n[^1]: Demystifying Satellite Assets for Post-Disaster Situation Awareness. World Bank via OpenDRI.org. Accessed November 28, 2018.","title":"Architecture"},{"location":"/getting-started.html","text":"","title":"Getting Started"},{"location":"/getting-started.html#getting-started","text":"Note If you are new to Earth-observing imagery, you might consider looking at the Concepts section first.\nThere are several ways to use RasterFrames, and several languages with which you can use it.\nThe simplest way to get started with RasterFrames is via the Docker image, or from the Python shell. To get started with the Python shell you will need:\nPython installed. Version 3.6 or greater is recommended. pip installed. If you are using Python 3, pip may already be installed. Java JDK 8 installed on your system and java on your system PATH or JAVA_HOME pointing to a Java installation.","title":"Getting Started"},{"location":"/getting-started.html#pip-install-pyrasterframes","text":"$ python3 -m pip install pyrasterframes\nThen in a python interpreter of your choice, you can get a pyspark SparkSession using the local[*] master.\nimport pyrasterframes\nspark = pyrasterframes.get_spark_session()\nThen, you can read a raster and work with it in a Spark DataFrame.\nfrom pyrasterframes.rasterfunctions import *\nfrom pyspark.sql.functions import lit\n\n# Read a MODIS surface reflectance granule\ndf = spark.read.raster('https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/MCD43A4.A2019059.h11v08.006.2019072203257_B02.TIF')\n\n# Add 3 element-wise, show some rows of the DataFrame\nsample = df.withColumn('added', rf_local_add(df.proj_raster, lit(3))) \\\n  .select(rf_crs('added'), rf_extent('added'), rf_tile('added'))\nsample\nShowing only top 5 rows.\nrf_crs(added) rf_extent(added) rf_tile(added) [+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs ] [-7783653.637667, 993342.4642358534, -7665045.582235853, 1111950.519667] [+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs ] [-7665045.582235853, 993342.4642358534, -7546437.526804706, 1111950.519667] [+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs ] [-7546437.526804707, 993342.4642358534, -7427829.47137356, 1111950.519667] [+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs ] [-7427829.47137356, 993342.4642358534, -7309221.415942413, 1111950.519667] [+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs ] [-7309221.415942414, 993342.4642358534, -7190613.360511267, 1111950.519667]\nThis example is extended in the getting started Jupyter notebook.","title":"pip install pyrasterframes"},{"location":"/getting-started.html#next-steps","text":"To understand more about how and why RasterFrames represents Earth observation in DataFrames, read about the core concepts and the project description. For more hands-on examples, see the chapters about reading and processing with RasterFrames.","title":"Next Steps"},{"location":"/getting-started.html#other-options","text":"You can also use RasterFrames in the following environments:\nJupyter Notebook pyspark shell","title":"Other Options"},{"location":"/getting-started.html#using-jupyter-notebook","text":"RasterFrames provides a Docker image for a Jupyter notebook server whose default kernel is already set up for running RasterFrames. To use it:\nInstall Docker Pull the image: docker pull s22s/rasterframes-notebook Run a container with the image, for example: docker run -p 8808:8888 -p 44040:4040 -v /path/to/notebooks:/home/jovyan/work rasterframes-notebook:latest In a browser, open localhost:8808 in the example above.\nSee the RasterFrames Notebook README for instructions on building the Docker image for this Jupyter notebook server.","title":"Using Jupyter Notebook"},{"location":"/getting-started.html#using-pyspark-shell","text":"You can use RasterFrames in a pyspark shell. To set up the pyspark environment, prepare your call with the appropriate --master and other --conf arguments for your cluster manager and environment. For RasterFrames support you need to pass arguments pointing to the various Java dependencies. You will also need the Python source zip, even if you have pip installed the package. You can download the source zip here: https://repo1.maven.org/maven2/org/locationtech/rasterframes/pyrasterframes_2.11/${VERSION}/pyrasterframes_2.11-${VERSION}-python.zip.\nThe pyspark shell command will look something like this.\npyspark \\\n    --master local[*] \\\n    --py-files pyrasterframes_2.11-${VERSION}-python.zip \\\n    --packages org.locationtech.rasterframes:rasterframes_2.11:${VERSION},org.locationtech.rasterframes:pyrasterframes_2.11:${VERSION},org.locationtech.rasterframes:rasterframes-datasource_2.11:${VERSION} \\\n    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\ # these configs improve serialization performance\n    --conf spark.kryo.registrator=org.locationtech.rasterframes.util.RFKryoRegistrator \\\n    --conf spark.kryoserializer.buffer.max=500m\nThen in the pyspark shell, import the module and call withRasterFrames on the SparkSession.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.2\n      /_/\n\nUsing Python version 3.7.3 (default, Mar 27 2019 15:43:19)\nSparkSession available as 'spark'.\n>>> import pyrasterframes\n>>> spark = spark.withRasterFrames()\n>>> df = spark.read.raster('https://landsat-pds.s3.amazonaws.com/c1/L8/158/072/LC08_L1TP_158072_20180515_20180604_01_T1/LC08_L1TP_158072_20180515_20180604_01_T1_B5.TIF')\nNow you have the configured SparkSession with RasterFrames enabled.","title":"Using pyspark shell"},{"location":"/getting-started.html#installing-gdal","text":"GDAL provides a wide variety of drivers to read data from many different raster formats. If GDAL is installed in the environment, RasterFrames will be able to read those formats. If you are using the Jupyter Notebook image, GDAL is already installed for you. Otherwise follow the instructions below. Version 2.4.1 or greater is required.","title":"Installing GDAL"},{"location":"/getting-started.html#installing-on-macos","text":"Using homebrew:\nbrew install gdal","title":"Installing on MacOS"},{"location":"/getting-started.html#installing-on-linux","text":"Using apt-get:\nsudo apt-get update\nsudo apt-get install gdal-bin","title":"Installing on Linux"},{"location":"/getting-started.html#testing-for-gdal","text":"gdalinfo --formats\nTo support GeoTIFF and JPEG2000 formats, you should look for the following drivers from the output above:\nGTiff -raster- (rw+vs): GeoTIFF JPEG2000 -raster,vector- (rwv): JPEG-2000 part 1 (ISO/IEC 15444-1), based on Jasper library\nDo the following to see if RasterFrames was able to find GDAL:\nfrom pyrasterframes.utils import gdal_version\nprint(gdal_version())\nThis will print out something like “GDAL x.y.z, released 20yy/mm/dd”. If it reports “not available”, then GDAL is not installed in a place where the RasterFrames runtime was able to find it. Please file an issue to get help resolving this problem.","title":"Testing For GDAL"},{"location":"/concepts.html","text":"","title":"Concepts"},{"location":"/concepts.html#concepts","text":"There are a number of Earth-observation (EO) concepts that crop up in the discussion of RasterFrames features. We’ll cover these briefly in the sections below. However, here are a few links providing a more extensive introduction to working with Earth observation data.\nFundamentals of Remote Sensing Newcomers Earth Observation Guide Earth Observation Markets and Applications","title":"Concepts"},{"location":"/concepts.html#raster","text":"A raster is a regular grid of numeric values. A raster can be thought of as an image, as is the case if the values in the grid represent brightness along a greyscale. More generally, a raster can measure many different phenomena or encode a variety of different discrete classifications.","title":"Raster"},{"location":"/concepts.html#cell","text":"A cell is a single row and column intersection in the raster grid. It is a single pixel in an image. A cell’s value often represents one sample from a sensor encoded as a scalar value associated with a specific location and time.","title":"Cell"},{"location":"/concepts.html#cell-type","text":"A numeric cell value may be encoded in a number of different computer numeric formats. There are typically three characteristics used to describe a cell type:\nword size (bit-width) signed vs unsigned integral vs floating-point\nThe most frequently encountered cell types in RasterFrames are below.\nName Abbreviation Description Range Byte int8 Signed 8-bit integral -128 to 127 Unsigned Byte uint8 Unsigned 8-bit integral 0 to 255 Short int16 Signed 16-bit integral -32,768 to 32,767 Unsigned Short uint16 Unsigned 16-bit integral 0 to 65,535 Int int32 Signed 32-bit integral -2,147,483,648 to 2,147,483,647 Unsigned Int uint32 Unsigned 32-bit integral 0 to 4,294,967,295 Float float32 32-bit floating-point -3.4028235E38 to 3.4028235E38 Double float64 64-bit floating-point -1.7976931348623157E308 to 1.7976931348623157E308\nSee the section on “NoData” Handling for additional discussion on cell types and more exhaustive coverage of available cell types.","title":"Cell Type"},{"location":"/concepts.html#nodata","text":"A “NoData” (or N/A) value is a specifically identified value for a cell type used to indicate the absence of data. See the section on “NoData” Handling for additional discussion on “NoData”.","title":"NoData"},{"location":"/concepts.html#scene","text":"A scene (or granule) is a discrete instance of EO raster data with a specific extent (region), date-time, and map projection (or CRS).","title":"Scene"},{"location":"/concepts.html#band","text":"A scene frequently defines many different measurements captured at the same date-time, over the same extent, and meant to be processed together. These different measurements are referred to as bands. The name comes from the varying bandwidths of light and electromagnetic radiation measured in many EO datasets.","title":"Band"},{"location":"/concepts.html#coordinate-reference-system-crs-","text":"A coordinate reference system (or spatial reference system) is a set of mathematical constructs used to translate locations on the three-dimensional surface of the earth to the two dimensional raster grid. A CRS typically accompanies any EO data so it can be precisely located.","title":"Coordinate Reference System (CRS)"},{"location":"/concepts.html#extent","text":"An extent (or bounding box) is a rectangular region specifying the geospatial coverage of a raster or tile, a two-dimensional array of cells within a single CRS.","title":"Extent"},{"location":"/concepts.html#tile","text":"A tile (sometimes called a “chip”) is a rectangular subset of a scene. As a scene is a raster, a tile is also a raster. A tile can conceptually be thought of as a two-dimensional array.\nSome EO data has many bands or channels. Within RasterFrames, this third dimension is handled across columns of the DataFrame, such that the tiles within DataFrames are all two-dimensional arrays.\nTiles are often square and the dimensions are some power of two, for example 256 by 256.\nThe tile is the primary discretization unit used in RasterFrames. The scene’s overall extent is carved up into smaller extents and spread across rows.","title":"Tile"},{"location":"/raster-io.html","text":"","title":"Raster Data I/O"},{"location":"/raster-io.html#raster-data-i-o","text":"The standard mechanism by which any data is brought in and out of a Spark Dataframe is the Spark SQL DataSource. RasterFrames provides specialized DataSources for geospatial raster data and maintains compatibility with existing general purpose DataSources, such as Parquet.\nCatalog Readers aws-pds-l8-catalog: built-in catalog over Landsat on AWS aws-pds-modis-catalog: built-in catalog over MODIS on AWS geotrellis-catalog: for enumerating GeoTrellis layers Raster Readers raster: the standard reader for most raster data, including single raster files or catalogs geotiff: a simplified reader for reading a single GeoTIFF file geotrellis: for reading a GeoTrellis layer Raster Writers geotiff: beta writer to GeoTiff file format geotrellis: creating a GeoTrellis layer parquet: general purpose writer for Parquet\nFurthermore, when in a Jupyter Notebook environment, you can view Tile and DataFrame samples.\nThere is also support for vector data for masking and data labeling.","title":"Raster Data I/O"},{"location":"/raster-catalogs.html","text":"","title":"Raster Catalogs"},{"location":"/raster-catalogs.html#raster-catalogs","text":"While interesting processing can be done on a single raster file, RasterFrames shines when catalogs of raster data are to be processed. In its simplest form, a catalog is a list of URLs referencing raster files. This list can be a Spark DataFrame, Pandas DataFrame, CSV file or CSV string. The catalog is input into the raster DataSource described in the next page, which creates tiles from the rasters at the referenced URLs.\nA catalog can have one or two dimensions:\nOne-D: A single column contains raster URLs across the rows. All referenced rasters represent the same band. For example, a column of URLs to Landsat 8 near-infrared rasters covering Europe. Each row represents different places and times. Two-D: Many columns contain raster URLs. Each column references the same band, and each row represents the same place and time. For example, red-, green-, and blue-band columns for scenes covering Europe. Each row represents a single scene with the same resolution, extent, CRS, etc across the row.","title":"Raster Catalogs"},{"location":"/raster-catalogs.html#creating-a-catalog","text":"This section will provide some examples of catalogs creation, as well as introduce some experimental catalogs built into RasterFrames. Reading raster data represented by a catalog is covered in more detail in the next page.","title":"Creating a Catalog"},{"location":"/raster-catalogs.html#one-d","text":"A single URL is the simplest form of a catalog.\nfile_uri = \"/data/raster/myfile.tif\"\n# Pandas DF\nmy_cat = pd.DataFrame({'B01': [file_uri]})\n\n# equivalent Spark DF\nfrom pyspark.sql import Row\nmy_cat = spark.createDataFrame([Row(B01=file_uri)])\n\n#equivalent CSV string\nmy_cat = \"B01\\n{}\".format(file_uri)\nA single column represents the same content type with different observations along the rows. In this example it is band 1 of MODIS surface reflectance, which is visible red. In the example the location of the images is the same, indicated by the granule identifier h04v09, but the dates differ: 2018185 (July 4, 2018) and 2018188 (July 7, 2018).\nscene1_B01 = \"https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B01.TIF\"\nscene2_B01 = \"https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018188/MCD43A4.A2018188.h04v09.006.2018198232008_B01.TIF\"\n\n# a pandas DF\none_d_cat_pd = pd.DataFrame({'B01': [scene1_B01, scene2_B01]})\n\n# equivalent spark DF\none_d_cat_df = spark.createDataFrame([Row(B01=scene1_B01), Row(B01=scene2_B01)])\n\n# equivalent CSV string\none_d_cat_csv = '\\n'.join(['B01', scene1_B01, scene2_B01])\nThis is what it looks like in DataFrame form:\none_d_cat_df\nB01 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018188/MCD43A4.A2018188.h04v09.006.2018198232008_B01.TIF","title":"One-D"},{"location":"/raster-catalogs.html#two-d","text":"In this example, multiple columns representing multiple content types (bands) across multiple scenes. In each row, the scene is the same: granule id h04v09 on July 4 or July 7, 2018. The first column is band 1, red, and the second is band 2, near infrared.\nscene1_B01 = \"https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B01.TIF\"\nscene1_B02 = \"https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B02.TIF\"\nscene2_B01 = \"https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018188/MCD43A4.A2018188.h04v09.006.2018198232008_B01.TIF\"\nscene2_B02 = \"https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018188/MCD43A4.A2018188.h04v09.006.2018198232008_B02.TIF\"\n\n# Pandas DF\ntwo_d_cat_pd = pd.DataFrame([\n    {'B01': [scene1_B01], 'B02': [scene1_B02]},\n    {'B01': [scene2_B01], 'B02': [scene2_B02]}\n])     \n\n# or\ntwo_d_cat_df = spark.createDataFrame([\n    Row(B01=scene1_B01, B02=scene1_B02),\n    Row(B01=scene2_B01, B02=scene2_B02)\n])\n    \n# As CSV string\ntow_d_cat_csv = '\\n'.join(['B01,B02', scene1_B01 + \",\" + scene1_B02, scene2_B01 + \",\" + scene2_B02])\nThis is what it looks like in DataFrame form:\ntwo_d_cat_df\nB01 B02 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018188/MCD43A4.A2018188.h04v09.006.2018198232008_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018188/MCD43A4.A2018188.h04v09.006.2018198232008_B02.TIF","title":"Two-D"},{"location":"/raster-catalogs.html#using-external-catalogs","text":"The concept of a catalog is much more powerful when we consider examples beyond constructing the DataFrame, and instead read the data from an external source. Here’s an extended example of reading a cloud-hosted CSV file containing MODIS scene metadata and transforming it into a catalog. The metadata describing the content of each URL is an important aspect of processing raster data.\nfrom pyspark import SparkFiles\nfrom pyspark.sql import functions as F\n\nspark.sparkContext.addFile(\"https://modis-pds.s3.amazonaws.com/MCD43A4.006/2018-07-04_scenes.txt\")\n\nscene_list = spark.read \\\n    .format(\"csv\") \\\n    .option(\"header\", \"true\") \\\n    .load(SparkFiles.get(\"2018-07-04_scenes.txt\"))\nscene_list\nShowing only top 5 rows.\ndate download_url gid 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/index.html MCD43A4.A2018185.h04v09.006.2018194032851 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/index.html MCD43A4.A2018185.h01v09.006.2018194032819 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/index.html MCD43A4.A2018185.h06v03.006.2018194032807 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/index.html MCD43A4.A2018185.h03v09.006.2018194032826 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/index.html MCD43A4.A2018185.h08v09.006.2018194032839\nObserve the scenes list file has URIs to index.html files in the download_url column. The image URI’s are in the same directory. The filenames are of the form ${gid}_B${band}.TIF. The next code chunk builds these URIs, which completes our catalog.\nmodis_catalog = scene_list \\\n    .withColumn('base_url',\n        F.concat(F.regexp_replace('download_url', 'index.html$', ''), 'gid',)\n    ) \\\n    .withColumn('B01' , F.concat('base_url', F.lit(\"_B01.TIF\"))) \\\n    .withColumn('B02' , F.concat('base_url', F.lit(\"_B02.TIF\"))) \\\n    .withColumn('B03' , F.concat('base_url', F.lit(\"_B03.TIF\")))\nmodis_catalog\nShowing only top 5 rows.\ndate download_url gid base_url B01 B02 B03 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/index.html MCD43A4.A2018185.h04v09.006.2018194032851 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B03.TIF 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/index.html MCD43A4.A2018185.h01v09.006.2018194032819 https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819 https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819_B03.TIF 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/index.html MCD43A4.A2018185.h06v03.006.2018194032807 https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807 https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807_B03.TIF 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/index.html MCD43A4.A2018185.h03v09.006.2018194032826 https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826 https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826_B03.TIF 2018-07-04 00:00:00 https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/index.html MCD43A4.A2018185.h08v09.006.2018194032839 https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839 https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839_B02.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839_B03.TIF","title":"Using External Catalogs"},{"location":"/raster-catalogs.html#using-built-in-catalogs","text":"RasterFrames comes with two experimental catalogs over the AWS PDS Landsat 8 and MODIS repositories. They are created by downloading the latest scene lists and building up the appropriate band URI columns as in the prior example.\nNote: The first time you run these may take some time, as the catalogs are large and have to be downloaded. However, they are cached and subsequent invocations should be faster.","title":"Using Built-in Catalogs"},{"location":"/raster-catalogs.html#modis","text":"modis_catalog = spark.read.format('aws-pds-modis-catalog').load()\nmodis_catalog.printSchema()\nroot\n |-- product_id: string (nullable = false)\n |-- acquisition_date: timestamp (nullable = false)\n |-- granule_id: string (nullable = false)\n |-- gid: string (nullable = false)\n |-- B01: string (nullable = true)\n |-- B01qa: string (nullable = true)\n |-- B02: string (nullable = true)\n |-- B02qa: string (nullable = true)\n |-- B03: string (nullable = true)\n |-- B03aq: string (nullable = true)\n |-- B04: string (nullable = true)\n |-- B04qa: string (nullable = true)\n |-- B05: string (nullable = true)\n |-- B05qa: string (nullable = true)\n |-- B06: string (nullable = true)\n |-- B06qa: string (nullable = true)\n |-- B07: string (nullable = true)\n |-- B07qa: string (nullable = true)","title":"MODIS"},{"location":"/raster-catalogs.html#landsat-8","text":"The Landsat 8 catalog includes a richer set of metadata describing the contents of each scene.\nl8 = spark.read.format('aws-pds-l8-catalog').load()\nl8.printSchema()\nroot\n |-- product_id: string (nullable = false)\n |-- entity_id: string (nullable = false)\n |-- acquisition_date: timestamp (nullable = false)\n |-- cloud_cover_pct: float (nullable = false)\n |-- processing_level: string (nullable = false)\n |-- path: short (nullable = false)\n |-- row: short (nullable = false)\n |-- bounds_wgs84: struct (nullable = false)\n |    |-- minX: double (nullable = false)\n |    |-- maxX: double (nullable = false)\n |    |-- minY: double (nullable = false)\n |    |-- maxY: double (nullable = false)\n |-- B1: string (nullable = true)\n |-- B2: string (nullable = true)\n |-- B3: string (nullable = true)\n |-- B4: string (nullable = true)\n |-- B5: string (nullable = true)\n |-- B6: string (nullable = true)\n |-- B7: string (nullable = true)\n |-- B8: string (nullable = true)\n |-- B9: string (nullable = true)\n |-- B10: string (nullable = true)\n |-- B11: string (nullable = true)\n |-- BQA: string (nullable = true)","title":"Landsat 8"},{"location":"/raster-read.html","text":"","title":"Reading Raster Data"},{"location":"/raster-read.html#reading-raster-data","text":"RasterFrames registers a DataSource named raster that enables reading of GeoTIFFs (and other formats when GDAL is installed) from arbitrary URIs. The raster DataSource operates on either a single raster file location or another DataFrame, called a catalog, containing pointers to many raster file locations.\nRasterFrames can also read from GeoTrellis catalogs and layers.","title":"Reading Raster Data"},{"location":"/raster-read.html#single-raster","text":"The simplest way to use the raster reader is with a single raster from a single URI or file. In the examples that follow we’ll be reading from a Sentinel-2 scene stored in an AWS S3 bucket.\nrf = spark.read.raster('https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/B02.tif')\nrf.printSchema()\nroot\n |-- proj_raster_path: string (nullable = false)\n |-- proj_raster: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\nThe file at the address above is a valid Cloud Optimized GeoTIFF (COG), which RasterFrames fully supports. RasterFrames will take advantage of the optimizations in the COG format to enable more efficient reading compared to non-COG GeoTIFFs.\nLet’s unpack the proj_raster column and look at the contents in more detail. It contains a CRS, a spatial extent measured in that CRS, and a two-dimensional array of numeric values called a tile.\ncrs = rf.select(rf_crs(\"proj_raster\").alias(\"value\")).first()\nprint(\"CRS\", crs.value.crsProj4)\nCRS +proj=utm +zone=17 +datum=WGS84 +units=m +no_defs\nparts = rf.select(\n    rf_extent(\"proj_raster\").alias(\"extent\"),\n    rf_tile(\"proj_raster\").alias(\"tile\")\n)\nparts\nShowing only top 5 rows.\nextent tile [699960.0, 4284660.0, 715320.0, 4300020.0] [715320.0, 4284660.0, 730680.0, 4300020.0] [730680.0, 4284660.0, 746040.0, 4300020.0] [746040.0, 4284660.0, 761400.0, 4300020.0] [761400.0, 4284660.0, 776760.0, 4300020.0]\nYou can also see that the single raster has been broken out into many arbitrary non-overlapping regions. Doing so takes advantage of parallel in-memory reads from the cloud hosted data source and allows Spark to work on manageable amounts of data per task. The following code fragment shows us how many subtiles were created from a single source image.\ncounts = rf.groupby(rf.proj_raster_path).count()\ncounts\nproj_raster_path count https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/B02.tif 64\nLet’s select a single tile and view it. The tile preview image as well as the string representation provide some basic information about the tile: its dimensions as numbers of columns and rows and the cell type, or data type of all the cells in the tile. For more about cell types, refer to this discussion.\ntile = rf.select(rf_tile(\"proj_raster\")).first()[0]\ndisplay(tile)","title":"Single Raster"},{"location":"/raster-read.html#uri-formats","text":"RasterFrames relies on three different I/O drivers, selected based on a combination of scheme, file extentions, and library availability. GDAL is used by default if a compatible version of GDAL (>= 2.4) is installed, and if GDAL supports the specified scheme. If GDAL is not available, either the Java I/O or Hadoop driver will be selected, depending on scheme.\nNote: The GDAL driver is the only one that can read non-GeoTIFF files.\nPrefix GDAL Java I/O Hadoop gdal://<vsidrv>// yes no no file:// yes yes no http:// yes yes no https:// yes yes no ftp:// /vsicurl/ yes no hdfs:// /vsihdfs/ no yes s3:// /vsis3/ yes no s3n:// no no yes s3a:// no no yes wasb:// /vsiaz/ no yes wasbs:// no no yes\nSpecific GDAL Virtual File System drivers can be selected using the gdal://<vsidrv>// syntax. For example If you have a archive.zip file containing a GeoTiff named my-file-inside.tif, you can address it with gdal://vsizip//path/to/archive.zip/my-file-inside.tif. Another example would be a MRF file in an S3 bucket on AWS: gdal://vsis3/my-bucket/prefix/to/raster.mrf. See the GDAL documentation for the format of the URIs after the gdal:/ scheme.","title":"URI Formats"},{"location":"/raster-read.html#raster-catalogs","text":"Consider the definition of a Catalog previously discussed, let’s read the raster data contained in the catalog URIs. We will start with the external catalog of MODIS surface reflectance.\nfrom pyspark import SparkFiles\nfrom pyspark.sql import functions as F\n\ncat_filename = \"2018-07-04_scenes.txt\"\nspark.sparkContext.addFile(\"https://modis-pds.s3.amazonaws.com/MCD43A4.006/{}\".format(cat_filename))\n\nmodis_catalog = spark.read \\\n    .format(\"csv\") \\\n    .option(\"header\", \"true\") \\\n    .load(SparkFiles.get(cat_filename)) \\\n    .withColumn('base_url',\n        F.concat(F.regexp_replace('download_url', 'index.html$', ''), 'gid')\n    ) \\\n    .drop('download_url') \\\n    .withColumn('red' , F.concat('base_url', F.lit(\"_B01.TIF\"))) \\\n    .withColumn('nir' , F.concat('base_url', F.lit(\"_B02.TIF\")))\n\nmodis_catalog.printSchema()\n\nprint(\"Available scenes: \", modis_catalog.count())\nroot\n |-- date: string (nullable = true)\n |-- gid: string (nullable = true)\n |-- base_url: string (nullable = true)\n |-- red: string (nullable = true)\n |-- nir: string (nullable = true)\n\nAvailable scenes:  297\nmodis_catalog\nShowing only top 5 rows.\ndate gid base_url red nir 2018-07-04 00:00:00 MCD43A4.A2018185.h04v09.006.2018194032851 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851 https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/04/09/2018185/MCD43A4.A2018185.h04v09.006.2018194032851_B02.TIF 2018-07-04 00:00:00 MCD43A4.A2018185.h01v09.006.2018194032819 https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819 https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/01/09/2018185/MCD43A4.A2018185.h01v09.006.2018194032819_B02.TIF 2018-07-04 00:00:00 MCD43A4.A2018185.h06v03.006.2018194032807 https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807 https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/06/03/2018185/MCD43A4.A2018185.h06v03.006.2018194032807_B02.TIF 2018-07-04 00:00:00 MCD43A4.A2018185.h03v09.006.2018194032826 https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826 https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/03/09/2018185/MCD43A4.A2018185.h03v09.006.2018194032826_B02.TIF 2018-07-04 00:00:00 MCD43A4.A2018185.h08v09.006.2018194032839 https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839 https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839_B01.TIF https://modis-pds.s3.amazonaws.com/MCD43A4.006/08/09/2018185/MCD43A4.A2018185.h08v09.006.2018194032839_B02.TIF\nMODIS data products are delivered on a regular, consistent grid, making identification of a specific area over time easy using (h,v) grid coordinates (see below).\nFor example, MODIS data right above the equator is all grid coordinates with v07.\nequator = modis_catalog.where(F.col('gid').like('%v07%')) \nequator.select('date', 'gid')\nShowing only top 5 rows.\ndate gid 2018-07-04 00:00:00 MCD43A4.A2018185.h07v07.006.2018194033213 2018-07-04 00:00:00 MCD43A4.A2018185.h08v07.006.2018194033224 2018-07-04 00:00:00 MCD43A4.A2018185.h09v07.006.2018194033547 2018-07-04 00:00:00 MCD43A4.A2018185.h26v07.006.2018194033904 2018-07-04 00:00:00 MCD43A4.A2018185.h12v07.006.2018194033912\nNow that we have prepared our catalog, we simply pass the DataFrame or CSV string to the raster DataSource to load the imagery. The catalog_col_names parameter gives the columns that contain the URI’s to be read.\nrf = spark.read.raster(\n    catalog=equator,\n    catalog_col_names=['red', 'nir']\n)\nrf.printSchema()\nroot\n |-- red_path: string (nullable = false)\n |-- nir_path: string (nullable = false)\n |-- red: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- nir: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- date: string (nullable = true)\n |-- gid: string (nullable = true)\n |-- base_url: string (nullable = true)\nObserve the schema of the resulting DataFrame has a projected raster struct for each column passed in catalog_col_names. For reference, the URI is now in a column appended with _path. Taking a quick look at the representation of the data, we see again each row contains an arbitrary portion of the entire scene coverage. We also see that for two-D catalogs, each row contains the same spatial extent for all tiles in that row.\nsample = rf \\\n    .select('gid', rf_extent('red'), rf_extent('nir'), rf_tile('red'), rf_tile('nir')) \\\n    .where(~rf_is_no_data_tile('red'))\nsample.limit(3)\ngid rf_extent(red) rf_extent(nir) rf_tile(red) rf_tile(nir) MCD43A4.A2018185.h07v07.006.2018194033213 [-1.16384154391778E7, 2105292.98390196, -1.151980738374676E7, 2223901.039333] [-1.16384154391778E7, 2105292.98390196, -1.151980738374676E7, 2223901.039333] MCD43A4.A2018185.h07v07.006.2018194033213 [-1.211284766090196E7, 1986684.9284709198, -1.199423960547092E7, 2105292.98390196] [-1.211284766090196E7, 1986684.9284709198, -1.199423960547092E7, 2105292.98390196] MCD43A4.A2018185.h07v07.006.2018194033213 [-1.175702349460884E7, 1986684.9284709198, -1.16384154391778E7, 2105292.98390196] [-1.175702349460884E7, 1986684.9284709198, -1.16384154391778E7, 2105292.98390196]","title":"Raster Catalogs"},{"location":"/raster-read.html#lazy-raster-reading","text":"By default, reading raster pixel values is delayed until it is absolutely needed. The DataFrame will contain metadata and pointers to the appropriate portion of the data until reading of the source raster data is absolutely necessary. This can save a significant of computation and I/O time for two reasons. First, a catalog may contain a large number of rows. Second, the raster DataSource attempts to apply spatial preciates (e.g. where/WHERE clauses with st_intersects, et al.) at row creation, reducing the chance of unneeded data being fetched.\nConsider the following two reads of the same data source. In the first, the lazy case, there is a pointer to the URI, extent and band to read. This will not be evaluated until the cell values are absolutely required. The second case shows the option to force the raster to be fully loaded right away.\nuri = 'https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/B02.tif'\nlazy = spark.read.raster(uri).select(col('proj_raster.tile').cast('string'))\nlazy\nShowing only top 5 rows.\ntile RasterRefTile(RasterRef(GDALRasterSource(https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/B02.tif),0,Some(Extent(699960.0, 4284660.0, 715320.0, 4300020.0)),Some(GridBounds(0,0,255,255)))) RasterRefTile(RasterRef(GDALRasterSource(https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/B02.tif),0,Some(Extent(715320.0, 4284660.0, 730680.0, 4300020.0)),Some(GridBounds(256,0,511,255)))) RasterRefTile(RasterRef(GDALRasterSource(https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/B02.tif),0,Some(Extent(730680.0, 4284660.0, 746040.0, 4300020.0)),Some(GridBounds(512,0,767,255)))) RasterRefTile(RasterRef(GDALRasterSource(https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/B02.tif),0,Some(Extent(746040.0, 4284660.0, 761400.0, 4300020.0)),Some(GridBounds(768,0,1023,255)))) RasterRefTile(RasterRef(GDALRasterSource(https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/B02.tif),0,Some(Extent(761400.0, 4284660.0, 776760.0, 4300020.0)),Some(GridBounds(1024,0,1279,255))))\nnon_lazy = spark.read.raster(uri, lazy_tiles=False).select(col('proj_raster.tile').cast('string'))\nnon_lazy\nShowing only top 5 rows.\ntile [uint16raw, (256,256), [533,521,519,538,553,462,443,423,403,467,…,243,246,252,243,248,246,255,245,252,254]] [uint16raw, (256,256), [269,265,262,278,268,264,272,277,303,308,…,266,243,243,238,233,242,278,290,285,303]] [uint16raw, (256,256), [211,221,213,231,265,325,320,235,252,346,…,291,345,330,336,311,328,332,344,375,404]] [uint16raw, (256,256), [222,218,221,223,220,219,213,211,215,209,…,246,297,341,326,629,340,377,482,481,323]] [uint16raw, (256,256), [302,307,311,261,211,204,223,217,218,210,…,410,272,245,207,233,231,228,236,242,226]]\nIn the initial examples on this page, you may have noticed that the realized (non-lazy) tiles are shown, but we did not change lazy_tiles. Instead, we used rf_tile to explicitly request the realized tile from the lazy representation.","title":"Lazy Raster Reading"},{"location":"/raster-read.html#multiband-rasters","text":"A multiband raster represents a three dimensional numeric array. The first two dimensions are spatial, and the third dimension is typically designated for different spectral bands. The bands could represent intensity of different wavelengths of light (or other electromagnetic radiation), or they could measure other phenomena such as time, quality indications, or additional gas concentrations, etc.\nMultiband rasters files have a strictly ordered set of bands, which are typically indexed from 1. Some files have metadata tags associated with each band. Some files have a color interpetation metadata tag indicating how to interpret the bands.\nWhen reading a multiband raster or a catalog describing multiband rasters, you will need to know ahead of time which bands you want to read. You will specify the bands to read, indexed from zero, as a list of integers into the band_indexes parameter of the raster reader.\nFor example, we can read a four-band (red, green, blue, and near-infrared) image as follows. The individual rows of the resulting DataFrame still represent distinct spatial extents, with a projected raster column for each band specified by band_indexes.\nmb = spark.read.raster(\n    's3://s22s-test-geotiffs/naip/m_3807863_nw_17_1_20160620.tif',\n    band_indexes=[0, 1, 2, 3],\n)\nmb.printSchema()\nroot\n |-- proj_raster_path: string (nullable = false)\n |-- proj_raster_b0: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- proj_raster_b1: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- proj_raster_b2: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- proj_raster_b3: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\nIf a band is passed into band_indexes that exceeds the number of bands in the raster, a projected raster column will still be generated in the schema but the column will be full of null values.\nYou can also pass a catalog and band_indexes together into the raster reader. This will create a projected raster column for the combination of all items passed into catalog_col_names and band_indexes. Again if a band in band_indexes exceeds the number of bands in a raster, it will have a null value for the corresponding column.\nHere is a trivial example with a catalog over multiband rasters. We specify two columns containing URIs and two bands, resulting in four projected raster columns.\nimport pandas as pd\nmb_cat = pd.DataFrame([\n    {'foo': 's3://s22s-test-geotiffs/naip/m_3807863_nw_17_1_20160620.tif',\n     'bar': 's3://s22s-test-geotiffs/naip/m_3807863_nw_17_1_20160620.tif'\n    },\n])\nmb2 = spark.read.raster(\n    catalog=spark.createDataFrame(mb_cat),\n    catalog_col_names=['foo', 'bar'],\n    band_indexes=[0, 1],\n    tile_dimensions=(64,64)\n)\nmb2.printSchema()\nroot\n |-- foo_path: string (nullable = false)\n |-- bar_path: string (nullable = false)\n |-- foo_b0: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- foo_b1: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- bar_b0: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- bar_b1: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)","title":"Multiband Rasters"},{"location":"/raster-read.html#geotrellis","text":"","title":"GeoTrellis"},{"location":"/raster-read.html#geotrellis-catalogs","text":"GeoTrellis is one of the key libraries upon which RasterFrames is built. It provides a Scala language API for working with geospatial raster data. GeoTrellis defines a tile layer storage format for persisting imagery mosaics. RasterFrames provides a DataSource supporting both reading and writing GeoTrellis layers.\nA GeoTrellis catalog is a set of GeoTrellis layers. We can read a DataFrame giving details of the content of a catalog using the syntax below. The scheme is typically hdfs or file, or a cloud storage provider like s3.\ngt_cat = spark.read.geotrellis_catalog('scheme://path-to-gt-catalog')","title":"GeoTrellis Catalogs"},{"location":"/raster-read.html#geotrellis-layers","text":"The catalog will give details on the individual layers available for query. We can read each layer with the URI to the catalog, the layer name, and the desired zoom level.\ngt_layer = spark.read.geotrellis(path='scheme://path-to-gt-catalog', layer=layer_name, zoom=zoom_level)\nThis will return a RasterFrame with additional GeoTrellis-specific metadata, inherited from GeoTrellis, stored as JSON in the metadata of the tile column.","title":"GeoTrellis Layers"},{"location":"/raster-write.html","text":"","title":"Writing Raster Data"},{"location":"/raster-write.html#writing-raster-data","text":"RasterFrames is oriented toward large scale analyses of spatial data. The primary output of these analyses could be a statistical summary, a machine learning model, or some other result that is generally much smaller than the input dataset.\nHowever, there are times in any analysis where writing a representative sample of the work in progress provides valuable feedback on the current state of the process and results.","title":"Writing Raster Data"},{"location":"/raster-write.html#tile-samples","text":"We have some convenience methods to quickly visualize _tile_s (see discussion of the RasterFrame schema for orientation to the concept) when inspecting a subset of the data in a Notebook.\nIn an IPython or Jupyter interpreter, a Tile object will be displayed as an image with limited metadata.\ndef scene(band):\n    b = str(band).zfill(2) # converts int 2 to '02'\n    return 'https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/' \\\n             'MCD43A4.A2019059.h11v08.006.2019072203257_B{}.TIF'.format(b)\nspark_df = spark.read.raster(scene(2), tile_dimensions=(128, 128))\ntile = spark_df.select(rf_tile('proj_raster').alias('tile')).first()['tile']\ntile","title":"Tile Samples"},{"location":"/raster-write.html#dataframe-samples","text":"Within an IPython or Jupyter interpreter, a Spark and Pandas DataFrames containing a column of tiles will be rendered as the samples discussed above. Simply import the rf_ipython submodule to enable enhanced HTML rendering of these DataFrame types.\nimport pyrasterframes.rf_ipython\n\nsamples = spark_df \\\n    .select(\n        rf_extent('proj_raster').alias('extent'),\n        rf_tile('proj_raster').alias('tile'),\n    )\\\n    .select('extent.*', 'tile') \\\n    .limit(3)\nsamples\nxmin ymin xmax ymax tile -7783653.637667 1052646.4919514267 -7724349.609951427 1111950.519667 -7724349.609951427 1052646.4919514267 -7665045.582235854 1111950.519667 -7665045.582235853 1052646.4919514267 -7605741.55452028 1111950.519667","title":"DataFrame Samples"},{"location":"/raster-write.html#geotiffs","text":"GeoTIFF is one of the most common file formats for spatial data, providing flexibility in data encoding, representation, and storage. RasterFrames provides a specialized Spark DataFrame writer for rendering a RasterFrame to a GeoTIFF.\nOne downside to GeoTIFF is that it is not a big data native format. To create a GeoTIFF, all the data to be encoded has to be in the memory of one computer (in Spark parlance, this is a “collect”), limiting it’s maximum size substantially compared to that of a full cluster environment. When rendering GeoTIFFs in RasterFrames, you must either specify the dimensions of the output raster, or deliberately limit the size of the collected data.\nFortunately, we can use the cluster computing capability to downsample the data into a more manageable size. For sake of example, let’s render an overview of a scene’s red band as a small raster, reprojecting it to latitude and longitude coordinates on the WGS84 reference ellipsoid (aka EPSG:4326).\noutfile = os.path.join('/tmp', 'geotiff-overview.tif')\nspark_df.write.geotiff(outfile, crs='EPSG:4326', raster_dimensions=(256, 256))\nWe can view the written file with rasterio:\nimport rasterio\nfrom rasterio.plot import show, show_hist\n\nwith rasterio.open(outfile) as src:\n    # View raster\n    show(src, adjust='linear')\n    # View data distribution\n    show_hist(src, bins=50, lw=0.0, stacked=False, alpha=0.6,\n        histtype='stepfilled', title=\"Overview Histogram\")\nIf there are many tile or projected raster columns in the DataFrame, the GeoTIFF writer will write each one as a separate band in the file. Each band in the output will be tagged the input column names for reference.","title":"GeoTIFFs"},{"location":"/raster-write.html#geotrellis-layers","text":"GeoTrellis is one of the key libraries upon which RasterFrames is built. It provides a Scala language API for working with geospatial raster data. GeoTrellis defines a tile layer storage format for persisting imagery mosaics. RasterFrames can write data from a RasterFrameLayer into a GeoTrellis Layer. RasterFrames provides a geotrellis DataSource that supports both reading and writing GeoTrellis layers.\nAn example is forthcoming. In the mean time referencing the GeoTrellisDataSourceSpec test code may help.","title":"GeoTrellis Layers"},{"location":"/raster-write.html#parquet","text":"You can write a RasterFrame to the Apache Parquet format. This format is designed to efficiently persist and query columnar data in distributed file system, such as HDFS. It also provides benefits when working in single node (or “local”) mode, such as tailoring organization for defined query patterns.\nspark_df.withColumn('exp', rf_expm1('proj_raster')) \\\n    .write.mode('append').parquet('hdfs:///rf-user/sample.pq')","title":"Parquet"},{"location":"/vector-data.html","text":"","title":"Vector Data"},{"location":"/vector-data.html#vector-data","text":"RasterFrames provides a variety of ways to work with spatial vector data (points, lines, and polygons) alongside raster data. There is a convenience DataSource for the GeoJSON format, as well as the ability to convert from GeoPandas to Spark. Representation of vector geometries in PySpark is through Shapely, providing a great deal of interoperability. RasterFrames also provides access to Spark functions for working with geometries.","title":"Vector Data"},{"location":"/vector-data.html#geojson-datasource","text":"from pyspark import SparkFiles\nspark.sparkContext.addFile('https://raw.githubusercontent.com/datasets/geo-admin1-us/master/data/admin1-us.geojson')\n\ndf = spark.read.geojson(SparkFiles.get('admin1-us.geojson'))\ndf.printSchema()\nroot\n |-- geometry: geometry (nullable = true)\n |-- ISO3166-1-Alpha-3: string (nullable = true)\n |-- country: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- state_code: string (nullable = true)\nThe properties of each discrete geometry are available as columns of the DataFrame, along with the geometry itself.","title":"GeoJSON DataSource"},{"location":"/vector-data.html#geopandas-and-rasterframes","text":"You can also convert a GeoPandas GeoDataFrame to a Spark DataFrame, preserving the geometry column. This means that any vector format that can be read with OGR can be converted to a Spark DataFrame. In the example below, we expect the same schema as the DataFrame defined above by the GeoJSON reader. Note that in a GeoPandas DataFrame there can be heterogeneous geometry types in the column, which may fail Spark’s schema inference.\nimport geopandas\nfrom shapely.geometry import MultiPolygon\n\ndef poly_or_mp_to_mp(g):\n    \"\"\" Normalize polygons or multipolygons to all be multipolygons. \"\"\"\n    if isinstance(g, MultiPolygon):\n        return g\n    else:\n        return MultiPolygon([g])\n\ngdf = geopandas.read_file('https://raw.githubusercontent.com/datasets/geo-admin1-us/master/data/admin1-us.geojson')\ngdf.geometry = gdf.geometry.apply(poly_or_mp_to_mp)\ndf2 = spark.createDataFrame(gdf)\ndf2.printSchema()\nroot\n |-- name: string (nullable = true)\n |-- country: string (nullable = true)\n |-- ISO3166-1-Alpha-3: string (nullable = true)\n |-- state_code: string (nullable = true)\n |-- id: string (nullable = true)\n |-- geometry: struct (nullable = true)\n |    |-- __geom__: long (nullable = true)\n |    |-- _is_empty: boolean (nullable = true)\n |    |-- _ndim: long (nullable = true)","title":"GeoPandas and RasterFrames"},{"location":"/vector-data.html#shapely-geometry-support","text":"The geometry column will have a Spark user-defined type that is compatible with Shapely when working with Python via PySpark. This means that when the data is collected to the driver, it will be a Shapely geometry object.\nthe_first = df.first()\nprint(type(the_first['geometry']))\n<class 'shapely.geometry.polygon.Polygon'>\nSince it is a geometry we can do things like this:\nthe_first['geometry'].wkt\n'POLYGON ((-71.14789974636884 41.64758738867177, -71.1203820461734 41.49465098730397, -71.85382564969197 41.32003632258973, -71.79295081245215 41.46661652278563, -71.8009089830251 42.01324982356905, -71.37915178087496 42.02436025651181, -71.30507361518457 41.76241242122431, -71.14789974636884 41.64758738867177))'\nYou can also write user-defined functions that take geometries as input, output, or both, via user defined types in the geomesa_pyspark.types module. Here is a simple but inefficient example of a user-defined function that uses both a geometry input and output to compute the centroid of a geometry. Observe in a sample of the data the geometry columns print as well known text (wkt).\nfrom pyspark.sql.functions import udf\nfrom geomesa_pyspark.types import PointUDT\n\n@udf(PointUDT())\ndef inefficient_centroid(g):\n    return g.centroid\n\ndf.select(df.state_code, inefficient_centroid(df.geometry))\nShowing only top 5 rows.\nstate_code inefficient_centroid(geometry) RI POINT (-71.52928731203043 41.68199156750224) FL POINT (-82.50257410032164 28.61692830512151) OK POINT (-97.5041982257254 35.58149601643106) MN POINT (-94.17743611908642 46.360073163397246) TX POINT (-99.3286756609654 31.45550825618542)","title":"Shapely Geometry Support"},{"location":"/vector-data.html#geomesa-functions-and-spatial-relations","text":"As documented in the function reference, various user-defined functions implemented by GeoMesa are also available for use. The example below uses a GeoMesa user-defined function to compute the centroid of a geometry. It is logically equivalent to the example above, but more efficient.\nfrom pyrasterframes.rasterfunctions import st_centroid\ndf.select(df.state_code, inefficient_centroid(df.geometry), st_centroid(df.geometry))\nShowing only top 5 rows.\nstate_code inefficient_centroid(geometry) st_centroid(geometry) RI POINT (-71.52928731203043 41.68199156750224) POINT (-71.52928731203043 41.68199156750224) FL POINT (-82.50257410032164 28.61692830512151) POINT (-82.50257410032164 28.61692830512151) OK POINT (-97.5041982257254 35.58149601643106) POINT (-97.5041982257254 35.58149601643106) MN POINT (-94.17743611908642 46.360073163397246) POINT (-94.17743611908642 46.360073163397246) TX POINT (-99.3286756609654 31.45550825618542) POINT (-99.3286756609654 31.45550825618542)\nThe RasterFrames vector functions and GeoMesa functions also provide a variety of spatial relations that are useful in combination with the geometric properties of projected rasters. In this example, we use the built-in Landsat catalog which provides an extent. We will convert the extent to a polygon and filter to those within approximately 500 km of a selected point.\nfrom pyrasterframes.rasterfunctions import st_geometry, st_bufferPoint, st_intersects, st_point\nfrom pyspark.sql.functions import lit\nl8 = spark.read.format('aws-pds-l8-catalog').load()\n\nl8 = l8.withColumn('geom', st_geometry(l8.bounds_wgs84))\nl8 = l8.withColumn('paducah', st_point(lit(-88.6275), lit(37.072222)))\n\nl8_filtered = l8.filter(st_intersects(l8.geom, st_bufferPoint(l8.paducah, lit(500000.0))))\nl8_filtered.select('product_id', 'entity_id', 'acquisition_date', 'cloud_cover_pct')\nShowing only top 5 rows.\nproduct_id entity_id acquisition_date cloud_cover_pct LC08_L1TP_018036_20130715_20170309_01_T1 LC80180362013196LGN01 2013-07-15 16:08:10.015 63.65 LC08_L1TP_026033_20170312_20170317_01_T1 LC80260332017071LGN00 2017-03-12 16:54:11.155 90.18 LC08_L1TP_020033_20170419_20170501_01_T1 LC80200332017109LGN00 2017-04-19 16:16:45.743 59.59 LC08_L1TP_024033_20170415_20170501_01_T1 LC80240332017105LGN00 2017-04-15 16:41:31.58 24.8 LC08_L1TP_027033_20170709_20170717_01_T1 LC80270332017190LGN00 2017-07-09 17:00:24.996 2.19","title":"GeoMesa Functions and Spatial Relations"},{"location":"/raster-processing.html","text":"","title":"Raster Processing"},{"location":"/raster-processing.html#raster-processing","text":"Explore what you can do with RasterFrames to work with, analyze, manipulate, and summarize Earth observation data.\nLocal Map Algebra “NoData” Handling Aggregation Time Series Machine Learning","title":"Raster Processing"},{"location":"/local-algebra.html","text":"","title":"Local Map Algebra"},{"location":"/local-algebra.html#local-map-algebra","text":"Local map algebra raster operations are element-wise operations on a single tile, between a tile and a scalar, between two tiles, or among many tiles. These operations are common in processing of earth observation imagery and other image data.\nCredit: GISGeography","title":"Local Map Algebra"},{"location":"/local-algebra.html#computing-ndvi","text":"Here is an example of computing the Normalized Differential Vegetation Index (NDVI). NDVI is a vegetation index which emphasizes differences in relative biomass and vegetation health. The term index in Earth observation means a combination of many raster bands into a single band that highlights a phenomenon of interest. Various indices have proven useful visual tools and frequently appear as features in machine learning models using Earth observation data.\nNDVI is often used worldwide to monitor drought, monitor and predict agricultural production, assist in predicting hazardous fire zones, and map desert encroachment. The NDVI is preferred for global vegetation monitoring because it helps to compensate for changing illumination conditions, surface slope, aspect, and other extraneous factors (Lillesand. Remote sensing and image interpretation. 2004)\nWe will apply the catalog pattern for defining the data we wish to process. To compute NDVI we need to compute local algebra on the red and near infrared (nir) bands:\nnir - red\nNDVI = ---------\n       nir + red\nThis form of (x - y) / (x + y) is common in remote sensing and is called a normalized difference. It is used with other band pairs to highlight water, snow, and other phenomena.\nfrom pyspark.sql import Row\nuri_pattern = 'https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/B0{}.tif'\ncatalog_df = spark.createDataFrame([\n    Row(red=uri_pattern.format(4), nir=uri_pattern.format(8))\n])\ndf = spark.read.raster(\n    catalog=catalog_df,\n    catalog_col_names=['red', 'nir']\n)\ndf.printSchema()\nroot\n |-- red_path: string (nullable = false)\n |-- nir_path: string (nullable = false)\n |-- red: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- nir: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\nObserve how the bands we need to use to compute the index are arranged as columns of our resulting DataFrame. The rows or observations are various spatial extents within the entire coverage area.\nRasterFrames provides a wide variety of local map algebra functions. There are several different broad categories, based on how many tiles the function takes as input:\nA function on a single tile is a unary operation; example: rf_log; A function on two tiles is a binary operation; example: rf_local_multiply; A function on a tile and a scalar is a binary operation; example: rf_local_less; or A function on many tiles is a n-ary operation; example: rf_agg_local_min\nWe can express the normalized difference with a combination of rf_local_divide, rf_local_subtract, and rf_local_add. Since the normalized difference is so common, there is a convenience method rf_normalized_difference, which we use in this example. We will append a new column to the DataFrame, which will apply the map alegbra function to each row.\ndf = df.withColumn('ndvi', rf_normalized_difference(df.nir, df.red))\ndf.printSchema()\nroot\n |-- red_path: string (nullable = false)\n |-- nir_path: string (nullable = false)\n |-- red: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- nir: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- ndvi: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\nWe can inspect a sample of the data. Yellow indicates very healthy vegetation, and purple represents bare soil or impervious surfaces.\nt = df.select(rf_tile('ndvi').alias('ndvi')).first()['ndvi']\ndisplay(t)\nWe continue examining NDVI in the time series section.","title":"Computing NDVI"},{"location":"/nodata-handling.html","text":"","title":"NoData Handling"},{"location":"/nodata-handling.html#handling","text":"","title":"“NoData” Handling"},{"location":"/nodata-handling.html#what-is-nodata-","text":"In raster operations, the preservation and correct processing of missing observations is very important. In most DataFrames and in scientific computing, the idea of missing data is expressed as a null or NaN value. However, a great deal of raster data is stored for space efficiency, which typically leads to use of integral values with a “sentinel” value designated to represent missing observations. This sentinel value varies across data products and is usually called the “NoData” value.\nRasterFrames provides a variety of functions to inspect and manage NoData within tiles.","title":"What is NoData?"},{"location":"/nodata-handling.html#cell-types","text":"To understand how NoData is handled in RasterFrames, we first need to understand the different underlying types of data called cell types. RasterFrames cell types are GeoTrellis CellTypes, so the GeoTrellis documentation is a valuable resource on how these are defined.\nThe CellType class from the rf_types submodule allows us to create a representation of any valid cell type. There are convenience methods to create instances for a variety of basic types.\nfrom pyrasterframes.rf_types import CellType\nCellType.bool()\nCellType.int8()\nCellType.uint8()\nCellType.int16()\nCellType.uint16()\nCellType.int32()\nCellType.float32()\nCellType.float64()\nWe can also inspect the cell type of a given tile or proj_raster column.\ncell_types = spark.read.raster('https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/B02.tif') \\\n    .select(rf_cell_type('proj_raster')).distinct()\ncell_types\nrf_cell_type(proj_raster) [uint16raw]","title":"Cell Types"},{"location":"/nodata-handling.html#understanding-cell-types-and-nodata","text":"We can use the methods on the CellType class to learn more about a specific cell type. Let’s consider the cell type of our sample data above.\nct = CellType('uint16raw')\nct, ct.is_floating_point(), ct.has_no_data()\n(uint16raw, False, False)\nWe can see that for the above data source, there is no defined NoData value. This means that each value is interpreted as a valid observation. Often such data is meant to be combined with another band indicating the quality of observations at each location. The lack of NoData is indicated by the raw at the end of the type name. Consider for contrast the uint16 cell type.\nfrom pyrasterframes.rf_types import CellType\nct = CellType('uint16')\nct, ct.is_floating_point(), ct.has_no_data(), ct.no_data_value()\n(uint16, False, True, 0)\nIn this case, the minimum value of 0 is designated as the NoData value. For integral-valued cell types, the NoData is typically zero, the maximum, or the minimum value for the underlying data type. The NoData value can also be a user-defined value. In that case the value is designated with ud.\nCellType.uint16().with_no_data_value(99).cell_type_name\n'uint16ud99'\nFloating point types have NaN as the NoData value by default. However, a user-defined NoData can be set.\nprint(CellType.float32().no_data_value())\nprint(CellType.float32().with_no_data_value(-99.9).no_data_value())\nnan\n-99.9","title":"Understanding Cell Types and NoData"},{"location":"/nodata-handling.html#masking","text":"Let’s continue the example above with Sentinel-2 data. Band 2 is blue and has no defined NoData. The quality information is in a separate file called the scene classification (SCL), which delineates areas of missing data and probable clouds. For more information on this, see the Sentinel-2 algorithm overview. Figure 3 tells us how to interpret the scene classification. For this example, we will exclude NoData, defective pixels, probable clouds, and cirrus clouds: values 0, 1, 8, 9, and 10.\nCredit: Sentinel-2 algorithm overview\nThe first step is to create a catalog with our band of interest and the SCL band. We read the data from the catalog, so the blue band and SCL tiles are aligned across rows.\nfrom pyspark.sql import Row\n\nblue_uri = 'https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/B02.tif'\nscl_uri = 'https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/SCL.tif'\ncat = spark.createDataFrame([Row(blue=blue_uri, scl=scl_uri),])\nunmasked = spark.read.raster(catalog=cat, catalog_col_names=['blue', 'scl'])\nunmasked.printSchema()\nroot\n |-- blue_path: string (nullable = false)\n |-- scl_path: string (nullable = false)\n |-- blue: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- scl: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\ncell_types = unmasked.select(rf_cell_type('blue'), rf_cell_type('scl')).distinct()\ncell_types\nrf_cell_type(blue) rf_cell_type(scl) [uint16raw] [uint8raw]\nDrawing on local map algebra techniques, we will create new tile columns that are indicators of unwanted pixels, as defined above. Since the mask column is an integer type, the addition is equivalent to a logical or, so the boolean true values are 1.\nfrom pyspark.sql.functions import lit\n\nmask_part = unmasked.withColumn('nodata', rf_local_equal('scl', lit(0))) \\\n                    .withColumn('defect', rf_local_equal('scl', lit(1))) \\\n                    .withColumn('cloud8', rf_local_equal('scl', lit(8))) \\\n                    .withColumn('cloud9', rf_local_equal('scl', lit(9))) \\\n                    .withColumn('cirrus', rf_local_equal('scl', lit(10)))\n\none_mask = mask_part.withColumn('mask', rf_local_add('nodata', 'defect')) \\\n                    .withColumn('mask', rf_local_add('mask', 'cloud8')) \\\n                    .withColumn('mask', rf_local_add('mask', 'cloud9')) \\\n                    .withColumn('mask', rf_local_add('mask', 'cirrus'))\n\ncell_types = one_mask.select(rf_cell_type('mask')).distinct()\ncell_types\nrf_cell_type(mask) [bool]\nBecause there is not a NoData already defined, we will choose one. In this particular example, the minimum value is greater than zero, so we can use 0 as the NoData value.\nblue_min = one_mask.agg(rf_agg_stats('blue').min.alias('blue_min'))\nblue_min\nblue_min 3.0\nWe can now construct the cell type string for our blue band’s cell type, designating 0 as NoData.\nblue_ct = one_mask.select(rf_cell_type('blue')).distinct().first()[0][0]\nmasked_blue_ct = CellType(blue_ct).with_no_data_value(0)\nmasked_blue_ct.cell_type_name\n'uint16ud0'\nNow we will use the rf_mask_by_value to designate the cloudy and other unwanted pixels as NoData in the blue column by converting the cell type and applying the mask.\nwith_nd = rf_convert_cell_type('blue', masked_blue_ct)\nmasked = one_mask.withColumn('blue_masked',\n    rf_mask_by_value(with_nd, 'mask', lit(1))) \\\n    .drop('nodata', 'defect', 'cloud8', 'cloud9', 'cirrus', 'blue')\nWe can verify that the number of NoData cells in the resulting blue_masked column matches the total of the boolean mask tile to ensure our logic is correct.\ncounts = masked.select(rf_no_data_cells('blue_masked'), rf_tile_sum('mask'))\ncounts\nShowing only top 5 rows.\nrf_no_data_cells(blue_masked) rf_tile_sum(mask) 4475 4475.0 0 0.0 0 0.0 0 0.0 5 5.0\nIt’s also nice to view a sample. The white regions are areas of NoData.\nsample = masked.orderBy(-rf_no_data_cells('blue_masked')).select(rf_tile('blue_masked'), rf_tile('scl')).first()\ndisplay(sample[0])\nAnd the original SCL data. The bright yellow is a cloudy region in the original image.\ndisplay(sample[1])","title":"Masking"},{"location":"/nodata-handling.html#nodata-and-local-arithmetic","text":"Let’s now explore how the presence of NoData affects local map algebra operations. To demonstrate the behavior, lets create two tiles. One tile will have values of 0 and 1, and the other will have values of just 0.\ntile_size = 100\nx = np.zeros((tile_size, tile_size), dtype='int16')\nx[:,tile_size//2:] = 1\nx = Tile(x)\ny = Tile(np.zeros((tile_size, tile_size), dtype='int16'))\n\nrf = spark.createDataFrame([Row(x=x, y=y)])\nprint('x')\ndisplay(x)\nx\nprint('y')\ndisplay(y)\ny\n/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py:897: UserWarning: Warning: converting a masked element to nan.\n  dtype = np.min_scalar_type(value)\n/anaconda3/lib/python3.6/site-packages/numpy/ma/core.py:710: UserWarning: Warning: converting a masked element to nan.\n  data = np.array(a, copy=False, subok=subok)\nNow, let’s create a new column from x with the value of 1 changed to NoData. Then, we will add this new column with NoData to the y column. As shown below, the result of the sum also has NoData (represented in white). In general for local algebra operations, Data + NoData = NoData.\nmasked_rf = rf.withColumn('x_nd', rf_mask_by_value('x', 'x', lit(1)) )\nmasked_rf = masked_rf.withColumn('x_nd_y_sum', rf_local_add('x_nd', 'y'))\nrow = masked_rf.collect()[0]\nprint('x with NoData')\ndisplay(row.x_nd)\nx with NoData\nprint('x with NoData plus y')\ndisplay(row.x_nd_y_sum)\nx with NoData plus y\nTo see more information about possible operations on tile columns, see the local map algebra page and function reference.","title":"NoData and Local Arithmetic"},{"location":"/nodata-handling.html#changing-a-tiles-nodata-values","text":"One way to mask a tile is to make a new tile with a user defined NoData value. We will explore this method below. First, lets create a DataFrame from a tile with values of 0, 1, 2, and 3. We will use numpy to create a 100x100 tile with vertical bands containing values 0, 1, 2, and 3.\ntile_size = 100\nx = np.zeros((tile_size, tile_size), dtype='int16')\n\n# setting the values of the columns\nfor i in range(4):\n    x[:, i*tile_size//4:(i+1)*tile_size//4] = i\nx = Tile(x)\n\nrf = spark.createDataFrame([Row(tile=x)])\ndisplay(x)\nFirst, we mask the value of 1 by making a new tile column with the user defined cell type ‘uint16ud1’. Then, we mask out the value of two by making a tile column with the cell type ‘uint16ud2’.\ndef get_nodata_ct(nd_val):\n\treturn CellType('uint16').with_no_data_value(nd_val)\n\nmasked_rf = rf.withColumn('tile_nd_1',\n                           rf_convert_cell_type('tile', get_nodata_ct(1))) \\\n              .withColumn('tile_nd_2',\n                          rf_convert_cell_type('tile_nd_1', get_nodata_ct(2))) \\\ncollected = masked_rf.collect()\nLet’s look at the new tiles we created. The tile named tile_nd_1 has the 1 values masked out as expected.\ndisplay(collected[0].tile_nd_1)\nAnd the tile named tile_nd_2 has the values of 1 and 2 masked out. This is because we created the tile by setting a new user defined NoData value to tile_nd_1, and the values previously masked out in tile_nd_1 stayed masked when creating tile_nd_2.\ndisplay(collected[0].tile_nd_2)","title":"Changing a Tile’s NoData Values"},{"location":"/nodata-handling.html#combining-tiles-with-different-data-types","text":"RasterFrames supports having tile columns with different cell types in a single DataFrame. It is important to understand how these different cell types interact.\nLet’s first create a RasterFrame that has columns of float and int cell type.\nx = Tile((np.ones((100, 100))*2), CellType.float64())\ny = Tile((np.ones((100, 100))*3), CellType.int32())\nrf = spark.createDataFrame([Row(x=x, y=y)])\n\ncell_types = rf.select(rf_cell_type('x'), rf_cell_type('y')).distinct()\ncell_types\nrf_cell_type(x) rf_cell_type(y) [float64] [int32]\nWhen performing a local operation between tile columns with cell types int and float, the resulting tile cell type will be float. In local algebra over two tiles of different “sized” cell types, the resulting cell type will be the larger of the two input tiles’ cell types.\nsums = rf.select(\n    rf_cell_type('x'),\n    rf_cell_type('y'),\n    rf_cell_type(rf_local_add('x', 'y')).alias('xy_sum'),\n    )\nsums\nrf_cell_type(x) rf_cell_type(y) xy_sum [float64] [int32] [float64]\nCombining tile columns of different cell types gets a little trickier when user defined NoData cell types are involved. Let’s create two tile columns: one with a NoData value of 1, and one with a NoData value of 2 (using our previously defined get_nodata_ct function).\nx_nd_1 = Tile((np.ones((100, 100))*3), get_nodata_ct(1))\nx_nd_2 = Tile((np.ones((100, 100))*3), get_nodata_ct(2))\nrf_nd = spark.createDataFrame([Row(x_nd_1=x_nd_1, x_nd_2=x_nd_2)])\nLet’s try adding the tile columns with different NoData values. When there is an inconsistent NoData value in the two columns, the NoData value of the right-hand side of the sum is kept. In this case, this means the result has a NoData value of 1.\nrf_nd_sum = rf_nd.withColumn('x_nd_sum', rf_local_add('x_nd_2', 'x_nd_1'))\ncell_types = rf_nd_sum.select(rf_cell_type('x_nd_sum')).distinct()\ncell_types\nrf_cell_type(x_nd_sum) [uint16ud1]\nReversing the order of the sum changes the NoData value of the resulting column to 2.\nrf_nd_sum = rf_nd.withColumn('x_nd_sum', rf_local_add('x_nd_1', 'x_nd_2'))\ncell_types = rf_nd_sum.select(rf_cell_type('x_nd_sum')).distinct()\ncell_types\nrf_cell_type(x_nd_sum) [uint16ud2]","title":"Combining Tiles with Different Data Types"},{"location":"/nodata-handling.html#nodata-values-in-aggregation","text":"Let’s use the same tile as before to demonstrate how NoData values affect tile aggregations.\ntile_size = 100\nx = np.zeros((tile_size, tile_size))\nfor i in range(4):\n    x[:, i*tile_size//4:(i+1)*tile_size//4] = i\nx = Tile(x, CellType.int16())\n\nrf = spark.createDataFrame([Row(tile=x)])\ndisplay(x)\nFirst we create the two new masked tile columns as before. One with only the value of 1 masked, and the other with and values of 1 and 2 masked.\nmasked_rf = rf.withColumn('tile_nd_1',\n                           rf_convert_cell_type('tile', get_nodata_ct(1))) \\\n              .withColumn('tile_nd_2',\n                          rf_convert_cell_type('tile_nd_1', get_nodata_ct(2)))\nThe results of rf_tile_sum vary on the tiles that were masked. This is because any cells with NoData values are ignored in the aggregation. Note that tile_nd_2 has the lowest sum, since it has the fewest amount of data cells.\nsums = masked_rf.select(rf_tile_sum('tile'), rf_tile_sum('tile_nd_1'), rf_tile_sum('tile_nd_2'))\nsums\nrf_tile_sum(tile) rf_tile_sum(tile_nd_1) rf_tile_sum(tile_nd_2) 15000.0 12500.0 7500.0","title":"NoData Values in Aggregation"},{"location":"/aggregation.html","text":"","title":"Aggregation"},{"location":"/aggregation.html#aggregation","text":"There are three types of aggregate functions: tile aggregate, DataFrame aggregate, and element-wise local aggregate. In the tile aggregate functions, we are computing a statistical summary per row of a tile column in a DataFrame. In the DataFrame aggregate functions, we are computing statistical summaries over all of the cell values and across all of the rows in the DataFrame or group. In the element-wise local aggregate functions, we are computing the element-wise statistical summary across a DataFrame or group of tiles.","title":"Aggregation"},{"location":"/aggregation.html#tile-mean-example","text":"We can illustrate aggregate differences by computing an aggregate mean. First, we create a sample DataFrame of 2 tiles. The tiles will contain normally distributed cell values with the first row’s mean at 1.0 and the second row’s mean at 3.0. For details on use of the Tile class see the page on numpy interoperability.\nfrom pyrasterframes.rf_types import Tile, CellType\n\nt1 = Tile(1 + 0.1 * np.random.randn(5,5), CellType('float64raw'))\n\nt1.cells  # display the array in the Tile\narray([[1.062, 1.097, 1.088, 1.034, 1.04 ],\n       [0.909, 1.04 , 0.898, 0.979, 0.933],\n       [1.188, 1.036, 1.122, 0.992, 1.108],\n       [0.955, 1.039, 0.833, 0.808, 1.044],\n       [1.077, 0.93 , 0.98 , 0.875, 1.008]])\nt5 = Tile(5 + 0.1 * np.random.randn(5,5), CellType('float64raw'))\nt5.cells\narray([[5.022, 5.131, 4.91 , 5.123, 4.884],\n       [4.975, 4.964, 4.943, 5.029, 4.898],\n       [4.909, 4.858, 4.985, 4.898, 4.968],\n       [4.826, 4.786, 4.902, 4.946, 5.005],\n       [4.861, 4.946, 4.841, 4.996, 4.91 ]])\nCreate a Spark DataFrame from the Tile objects.\nimport pyspark.sql.functions as F\nfrom pyspark.sql import Row\n\nrf = spark.createDataFrame([\n    Row(id=1, tile=t1),\n    Row(id=2, tile=t5)\n]).orderBy('id')\nWe use the rf_tile_mean function to compute the tile aggregate mean of cells in each row of column tile. The mean of each tile is computed separately, so the first mean is about 1.0 and the second mean is about 3.0. Notice that the number of rows in the DataFrame is the same before and after the aggregation.\nrf.select(F.col('id'), rf_tile_mean(F.col('tile')))\nid rf_tile_mean(tile) 1 1.002965314252178 2 4.9407551893912265\nWe use the rf_agg_mean function to compute the DataFrame aggregate, which averages values across the fifty cells in two rows. Note that only a single row is returned since the average is computed over the full DataFrame.\nrf.agg(rf_agg_mean(F.col('tile')))\nrf_agg_mean(tile) 2.971860251821702\nWe use the rf_agg_local_mean function to compute the element-wise local aggregate mean across the two rows. For this aggregation, we are computing the mean of one value of 1.0 and one value of 3.0 to arrive at the element-wise mean, but doing so twenty-five times, one for each position in the tile.\nTo compute an element-wise local aggregate, tiles need to have the same dimensions. In this case, both tiles have 5 rows and 5 columns. If we tried to compute an element-wise local aggregate over the DataFrame without equal tile dimensions, we would get a runtime error.\nrf.agg(rf_agg_local_mean('tile')) \\\n    .first()[0].cells.data  # display the contents of the Tile array\narray([[3.042, 3.114, 2.999, 3.078, 2.962],\n       [2.942, 3.002, 2.92 , 3.004, 2.915],\n       [3.049, 2.947, 3.054, 2.945, 3.038],\n       [2.891, 2.913, 2.867, 2.877, 3.025],\n       [2.969, 2.938, 2.911, 2.935, 2.959]])","title":"Tile Mean Example"},{"location":"/aggregation.html#cell-counts-example","text":"We can also count the total number of data and NoData cells over all the tiles in a DataFrame using rf_agg_data_cells and rf_agg_no_data_cells. There are ~3.8 million data cells and ~1.9 million NoData cells in this DataFrame. See the section on “NoData” handling for additional discussion on handling missing data.\nrf = spark.read.raster('https://s22s-test-geotiffs.s3.amazonaws.com/MCD43A4.006/11/05/2018233/MCD43A4.A2018233.h11v05.006.2018242035530_B02.TIF')\nstats = rf.agg(rf_agg_data_cells('proj_raster'), rf_agg_no_data_cells('proj_raster'))\nstats\nrf_agg_data_cells(proj_raster) rf_agg_no_data_cells(proj_raster) 3825959 1934041","title":"Cell Counts Example"},{"location":"/aggregation.html#statistical-summaries","text":"The statistical summary functions return a summary of cell values: number of data cells, number of NoData cells, minimum, maximum, mean, and variance, which can be computed as a tile aggregate, a DataFrame aggregate, or an element-wise local aggregate.\nThe rf_tile_stats function computes summary statistics separately for each row in a tile column as shown below.\nrf = spark.read.raster('https://s22s-test-geotiffs.s3.amazonaws.com/luray_snp/B02.tif')\nstats = rf.select(rf_tile_stats('proj_raster').alias('stats'))\n\nstats.printSchema()\nroot\n |-- stats: struct (nullable = true)\n |    |-- data_cells: long (nullable = false)\n |    |-- no_data_cells: long (nullable = false)\n |    |-- min: double (nullable = false)\n |    |-- max: double (nullable = false)\n |    |-- mean: double (nullable = false)\n |    |-- variance: double (nullable = false)\nstats.select('stats.min', 'stats.max', 'stats.mean', 'stats.variance')\nShowing only top 5 rows.\nmin max mean variance 199.0 5331.0 455.5312957763667 68728.44405841625 187.0 1911.0 310.51860046386815 13849.46165963797 181.0 1530.0 259.9732513427734 3501.4270036255507 170.0 2535.0 292.7441253662109 11137.773410618069 152.0 3641.0 290.0180511474609 14295.314752891418\nThe rf_agg_stats function aggregates over all of the tiles in a DataFrame and returns a statistical summary of all cell values as shown below.\nstats = rf.agg(rf_agg_stats('proj_raster').alias('stats')) \\\n    .select('stats.min', 'stats.max', 'stats.mean', 'stats.variance')\nstats\nmin max mean variance 3.0 12103.0 542.1327946489893 685615.201702677\nThe rf_agg_local_stats function computes the element-wise local aggregate statistical summary as shown below. The DataFrame used in the previous two code blocks has unequal tile dimensions, so a different DataFrame is used in this code block to avoid a runtime error.\nrf = spark.createDataFrame([\n    Row(id=1, tile=t1),\n    Row(id=3, tile=t1 * 3),\n    Row(id=5, tile=t1 * 5)\n]).agg(rf_agg_local_stats('tile').alias('stats'))\n    \nagg_local_stats = rf.select('stats.min', 'stats.max', 'stats.mean', 'stats.variance').collect()\n\nfor r in agg_local_stats:\n    for stat in r.asDict():\n        print(stat, ':\\n', r[stat], '\\n')\nmin :\n Tile(dimensions=[5, 5], cell_type=CellType(float64, nan), cells=\n[[1.0615886298606014 1.097172089502433 1.0878367712315677\n  1.0336369220329946 1.0396450980524634]\n [0.9093510234770061 1.0395164132142185 0.8977792623994758\n  0.9794104698605454 0.9331587620871188]\n [1.1880155149222722 1.0358509293579166 1.121951382553168\n  0.9919570151469949 1.1083157156275618]\n [0.9549053635290896 1.0393643655651559 0.8325609664771204\n  0.8083227627617129 1.0437751826921156]\n [1.0771332955918707 0.9295633311374505 0.9800925322192545\n  0.8748399663315599 1.0083890906727804]]) \n\nmax :\n Tile(dimensions=[5, 5], cell_type=CellType(float64, nan), cells=\n[[5.307943149303007 5.485860447512165 5.439183856157839 5.168184610164973\n  5.198225490262317]\n [4.546755117385031 5.197582066071092 4.4888963119973795\n  4.897052349302727 4.665793810435594]\n [5.940077574611361 5.179254646789583 5.60975691276584 4.959785075734974\n  5.541578578137809]\n [4.774526817645448 5.196821827825779 4.162804832385602 4.041613813808564\n  5.218875913460578]\n [5.3856664779593535 4.647816655687253 4.900462661096273\n  4.3741998316577995 5.041945453363902]]) \n\nmean :\n Tile(dimensions=[5, 5], cell_type=CellType(float64, nan), cells=\n[[3.1847658895818043 3.291516268507299 3.2635103136947037\n  3.100910766098984 3.11893529415739]\n [2.7280530704310184 3.1185492396426553 2.6933377871984274\n  2.938231409581636 2.7994762862613563]\n [3.5640465447668164 3.10755278807375 3.3658541476595034\n  2.9758710454409845 3.3249471468826854]\n [2.864716090587269 3.1180930966954676 2.497682899431361\n  2.4249682882851387 3.131325548076347]\n [3.231399886775612 2.7886899934123512 2.9402775966577637\n  2.6245198989946794 3.0251672720183413]]) \n\nvariance :\n Tile(dimensions=[5, 5], cell_type=CellType(float64, nan), cells=\n[[3.0052544507981587 3.210097583955026 3.1557035755827236\n  2.849080764239581 2.8822984797453763]\n [2.2051180903964767 2.8815849955780095 2.149353610652126\n  2.557986315926545 2.322094067359906]\n [3.763682303189416 2.8612990609377587 3.356733079501243\n  2.623943253064894 3.2756366013520903]\n [2.4315846754576604 2.88074209175109 1.848420701070177\n  1.7423618367966087 2.9052443520105573]\n [3.093909697260276 2.304234630920944 2.5615503245652036\n  2.0409199111760152 2.711596155167671]])","title":"Statistical Summaries"},{"location":"/aggregation.html#histogram","text":"The rf_tile_histogram function computes a count of cell values within each row of tile and outputs a bins array with the schema below. In the graph below, we have plotted each bin’s value on the x-axis and count on the y-axis for the tile in the first row of the DataFrame.\nimport matplotlib.pyplot as plt\n\nrf = spark.read.raster('https://s22s-test-geotiffs.s3.amazonaws.com/MCD43A4.006/11/05/2018233/MCD43A4.A2018233.h11v05.006.2018242035530_B02.TIF')\n\nhist_df = rf.select(rf_tile_histogram('proj_raster')['bins'].alias('bins'))\nhist_df.printSchema()\n\nbins_row = hist_df.first()\nvalues = [int(bin['value']) for bin in bins_row.bins]\ncounts = [int(bin['count']) for bin in bins_row.bins]\n\nplt.hist(values, weights=counts, bins=100)\nplt.show()\nroot\n |-- bins: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- value: double (nullable = false)\n |    |    |-- count: long (nullable = false)\nThe rf_agg_approx_histogram function computes a count of cell values across all of the rows of tile in a DataFrame or group. In the example below, the range of the y-axis is significantly wider than the range of the y-axis on the previous histogram since this histogram was computed for all cell values in the DataFrame.\nbins_list = rf.agg(\n    rf_agg_approx_histogram('proj_raster')['bins'].alias('bins')\n    ).collect()\nvalues = [int(row['value']) for row in bins_list[0].bins]\ncounts = [int(row['count']) for row in bins_list[0].bins]\n\nplt.hist(values, weights=counts, bins=100)\nplt.show()","title":"Histogram"},{"location":"/time-series.html","text":"","title":"Time Series"},{"location":"/time-series.html#time-series","text":"","title":"Time Series"},{"location":"/time-series.html#analysis-plan","text":"In this example, we will show how the flexibility of the DataFrame concept for raster data allows a simple and intuitive way to extract a time series from Earth observation data. We will start with our built-in MODIS data catalog.\ncat = spark.read.format('aws-pds-modis-catalog').load().repartition(200)\ncat.printSchema()\nroot\n |-- product_id: string (nullable = false)\n |-- acquisition_date: timestamp (nullable = false)\n |-- granule_id: string (nullable = false)\n |-- gid: string (nullable = false)\n |-- B01: string (nullable = true)\n |-- B01qa: string (nullable = true)\n |-- B02: string (nullable = true)\n |-- B02qa: string (nullable = true)\n |-- B03: string (nullable = true)\n |-- B03aq: string (nullable = true)\n |-- B04: string (nullable = true)\n |-- B04qa: string (nullable = true)\n |-- B05: string (nullable = true)\n |-- B05qa: string (nullable = true)\n |-- B06: string (nullable = true)\n |-- B06qa: string (nullable = true)\n |-- B07: string (nullable = true)\n |-- B07qa: string (nullable = true)\nWe will summarize the change in NDVI over 2018 in the Cuyahoga Valley National Park in Ohio, USA. First, we will retrieve open vector data delineating the park boundary from the US National Park Service’s LandsNet.","title":"Analysis Plan"},{"location":"/time-series.html#vector-data","text":"First we will get the vector data from LandsNet service by a REST query. The data is saved to a geojson file.\nimport requests\nnps_filepath = '/tmp/parks.geojson'\nnps_data_query_url = 'https://services1.arcgis.com/fBc8EJBxQRMcHlei/arcgis/rest/services/' \\\n                     'NPS_Park_Boundaries/FeatureServer/0/query' \\\n                     '?geometry=-82.451,41.075,-80.682,41.436&inSR=4326&outSR=4326&f=geojson'\nr = requests.get(nps_data_query_url)\nwith open(nps_filepath,'wb') as f:\n    f.write(r.content)\nm = folium.Map((41.25,-81.6), zoom_start=10).add_child(folium.GeoJson(nps_filepath))\nNow we read the park boundary vector data as a Spark DataFrame using the built-in geojson DataSource. The geometry is very detailed, and the EO cells are relatively coarse. To speed up the processing, the geometry is “simplified” by combining vertices within about 100 meters of each other. For more on this see the section on Shapely support in user defined functions.\npark_vector = spark.read.geojson(nps_filepath)\n\n@udf(MultiPolygonUDT())\ndef simplify(g, tol):\n    return g.simplify(tol)\n\npark_vector = park_vector.withColumn('geo_simp', simplify('geometry', lit(0.001))) \\\n                         .select('geo_simp') \\\n                         .hint('broadcast')","title":"Vector Data"},{"location":"/time-series.html#catalog-read","text":"The entire park boundary is contained in MODIS granule h11 v04. We will simply filter on this granule, rather than using a spatial relation. The time period selected should show the change in plant vigor as leaves emerge over the spring and into early summer.\npark_cat = cat \\\n            .filter(\n                    (cat.granule_id == 'h11v04') &\n                    (cat.acquisition_date > lit('2018-02-19')) &\n                    (cat.acquisition_date < lit('2018-07-01'))            \n                    ) \\\n            .crossJoin(park_vector)\n                \npark_cat.printSchema()\nroot\n |-- product_id: string (nullable = false)\n |-- acquisition_date: timestamp (nullable = false)\n |-- granule_id: string (nullable = false)\n |-- gid: string (nullable = false)\n |-- B01: string (nullable = true)\n |-- B01qa: string (nullable = true)\n |-- B02: string (nullable = true)\n |-- B02qa: string (nullable = true)\n |-- B03: string (nullable = true)\n |-- B03aq: string (nullable = true)\n |-- B04: string (nullable = true)\n |-- B04qa: string (nullable = true)\n |-- B05: string (nullable = true)\n |-- B05qa: string (nullable = true)\n |-- B06: string (nullable = true)\n |-- B06qa: string (nullable = true)\n |-- B07: string (nullable = true)\n |-- B07qa: string (nullable = true)\n |-- geo_simp: multipolygon (nullable = true)\nNow we have a catalog with several months of MODIS data for a single granule. However, the granule is larger than our park boundary. We will combine the park geometry with the catalog, and read only the bands of interest to compute NDVI, which we discussed in a previous section.\nWe then reproject the park geometry to the same CRS as the imagery. Then we will filter to only the tiles intersecting the park.\nraster_cols = ['B01', 'B02',] # red and near-infrared respectively\npark_rf = spark.read.raster(\n        catalog=park_cat.select(['acquisition_date', 'granule_id', 'geo_simp'] + raster_cols),\n        catalog_col_names=raster_cols) \\\n    .withColumn('park_native', st_reproject('geo_simp', lit('EPSG:4326'), rf_crs('B01'))) \\\n    .filter(st_intersects('park_native', rf_geometry('B01'))) \n\npark_rf.printSchema()\nroot\n |-- B01_path: string (nullable = false)\n |-- B02_path: string (nullable = false)\n |-- B01: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- B02: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- acquisition_date: timestamp (nullable = false)\n |-- granule_id: string (nullable = false)\n |-- geo_simp: multipolygon (nullable = true)\n |-- park_native: geometry (nullable = true)","title":"Catalog Read"},{"location":"/time-series.html#vector-and-raster-data-interaction","text":"Now we have the vector representation of the park boundary alongside the tiles of red and near infrared bands. Next, we need to create a tile representation of the park to allow us to limit the time series analysis to pixels within the park. This is similar to the masking operation demonstrated in NoData handling.\nWe do this using two transformations. The first one will reproject the park boundary from coordinates to the MODIS sinusoidal projection. The second one will create a new tile aligned with the imagery containing a value of 1 where the pixels are contained within the park and NoData elsewhere.\nrf_park_tile = park_rf \\\n    .withColumn('dims', rf_dimensions('B01')) \\\n    .withColumn('park_tile', rf_rasterize('park_native', rf_geometry('B01'), lit(1), 'dims.cols', 'dims.rows')) \\\n    .persist()\n\nrf_park_tile.printSchema()\nroot\n |-- B01_path: string (nullable = false)\n |-- B02_path: string (nullable = false)\n |-- B01: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- B02: struct (nullable = true)\n |    |-- tile_context: struct (nullable = false)\n |    |    |-- extent: struct (nullable = false)\n |    |    |    |-- xmin: double (nullable = false)\n |    |    |    |-- ymin: double (nullable = false)\n |    |    |    |-- xmax: double (nullable = false)\n |    |    |    |-- ymax: double (nullable = false)\n |    |    |-- crs: struct (nullable = false)\n |    |    |    |-- crsProj4: string (nullable = false)\n |    |-- tile: tile (nullable = false)\n |-- acquisition_date: timestamp (nullable = false)\n |-- granule_id: string (nullable = false)\n |-- geo_simp: multipolygon (nullable = true)\n |-- park_native: geometry (nullable = true)\n |-- dims: struct (nullable = true)\n |    |-- cols: short (nullable = false)\n |    |-- rows: short (nullable = false)\n |-- park_tile: tile (nullable = true)","title":"Vector and Raster Data Interaction"},{"location":"/time-series.html#create-time-series","text":"Next, we will compute NDVI as the normalized difference of near infrared (band 2) and red (band 1). The tiles are masked by the park_tile. We will then aggregate across the remaining values to arrive at an average NDVI for each week of the year. Note that the computation is creating a weighted average, which is weighted by the number of valid observations per week.\nfrom pyspark.sql.functions import col, year, weekofyear, month\nfrom pyspark.sql.functions import sum as sql_sum\n\nrf_ndvi = rf_park_tile \\\n    .withColumn('ndvi', rf_normalized_difference('B02', 'B01')) \\\n    .withColumn('ndvi_masked', rf_mask('ndvi', 'park_tile'))\n\ntime_series = rf_ndvi \\\n        .withColumn('ndvi_wt', rf_tile_sum('ndvi_masked')) \\\n        .withColumn('wt', rf_data_cells('ndvi_masked')) \\\n        .groupby(year('acquisition_date').alias('year'), weekofyear('acquisition_date').alias('week')) \\\n        .agg(sql_sum('ndvi_wt').alias('ndvi_wt_wk'), sql_sum('wt').alias('wt_wk')) \\\n        .withColumn('ndvi', col('ndvi_wt_wk') / col('wt_wk'))\n        \ntime_series.printSchema()\nroot\n |-- year: integer (nullable = false)\n |-- week: integer (nullable = false)\n |-- ndvi_wt_wk: double (nullable = true)\n |-- wt_wk: long (nullable = true)\n |-- ndvi: double (nullable = true)\nFinally, we will take a look at the NDVI over time.\nimport matplotlib.pyplot as plt\n\ntime_series_pdf = time_series.toPandas()\ntime_series_pdf = time_series_pdf.sort_values('week')\nplt.plot(time_series_pdf['week'], time_series_pdf['ndvi'], 'go-')\nplt.xlabel('Week of year, 2018')\nplt.ylabel('NDVI')\nplt.title('Cuyahoga Valley NP Green-up')\nText(0.5,1,'Cuyahoga Valley NP Green-up')\nWe can see two fairly clear elbows in the curve at week 17 and week 21, indicating the start and end of the green up period. Estimation of such parameters is one technique phenology researchers use to monitor changes in climate and environment.","title":"Create Time Series"},{"location":"/machine-learning.html","text":"","title":"Machine Learning"},{"location":"/machine-learning.html#machine-learning","text":"RasterFrames provides facilities to train and predict with a wide variety of machine learning models through Spark ML Pipelines. This library provides a variety of pipeline components for supervised learning, unsupervised learning, and data preparation that can be used to represent and repeatably conduct a variety of tasks in machine learning.\nThe following sections provide some examples on how to integrate these workflows with RasterFrames.\nUnsupervised Machine Learning Imports and Data Preparation Create ML Pipeline Fit the Model and Score Visualize Prediction Supervised Machine Learning Create and Read Raster Catalog Data Prep Masking Poor Quality Cells Create ML Pipeline Train the Model Model Evaluation Visualize Prediction","title":"Machine Learning"},{"location":"/unsupervised-learning.html","text":"","title":"Unsupervised Machine Learning"},{"location":"/unsupervised-learning.html#unsupervised-machine-learning","text":"In this example, we will demonstrate how to fit and score an unsupervised learning model with a sample of Landsat 8 data.","title":"Unsupervised Machine Learning"},{"location":"/unsupervised-learning.html#imports-and-data-preparation","text":"We import various Spark components needed to construct our Pipeline.\nimport pandas as pd\nfrom pyrasterframes import TileExploder\nfrom pyrasterframes.rasterfunctions import rf_assemble_tile, rf_crs, rf_extent, rf_tile, rf_dimensions\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml import Pipeline\nThe first step is to create a Spark DataFrame of our imagery data. To achieve that we will create a catalog DataFrame using the pattern from the I/O page. In the catalog, each row represents a distinct area and time, and each column is the URI to a band’s image product. The function resource_dir_uri gives a local file system path to the sample Landsat data. The resulting Spark DataFrame may have many rows per URI, with a column corresponding to each band.\nfilenamePattern = \"L8-B{}-Elkton-VA.tiff\"\ncatalog_df = pd.DataFrame([\n    {'b' + str(b): os.path.join(resource_dir_uri(), filenamePattern.format(b)) for b in range(1, 8)}\n])\ndf = spark.read.raster(catalog=catalog_df, catalog_col_names=catalog_df.columns)\ndf = df.select(\n    rf_crs(df.b1).alias('crs'),\n    rf_extent(df.b1).alias('extent'),\n    rf_tile(df.b1).alias('b1'),\n    rf_tile(df.b2).alias('b2'),\n    rf_tile(df.b3).alias('b3'),\n    rf_tile(df.b4).alias('b4'),\n    rf_tile(df.b5).alias('b5'),\n    rf_tile(df.b6).alias('b6'),\n    rf_tile(df.b7).alias('b7'),\n)\ndf.printSchema()\nroot\n |-- crs: struct (nullable = true)\n |    |-- crsProj4: string (nullable = false)\n |-- extent: struct (nullable = true)\n |    |-- xmin: double (nullable = false)\n |    |-- ymin: double (nullable = false)\n |    |-- xmax: double (nullable = false)\n |    |-- ymax: double (nullable = false)\n |-- b1: tile (nullable = true)\n |-- b2: tile (nullable = true)\n |-- b3: tile (nullable = true)\n |-- b4: tile (nullable = true)\n |-- b5: tile (nullable = true)\n |-- b6: tile (nullable = true)\n |-- b7: tile (nullable = true)","title":"Imports and Data Preparation"},{"location":"/unsupervised-learning.html#create-ml-pipeline","text":"SparkML requires that each observation be in its own row, and features for each observation be packed into a single Vector. For this unsupervised learning problem, we will treat each pixel as an observation and each band as a feature. The first step is to “explode” the tiles into a single row per pixel. In RasterFrames, generally a pixel is called a cell.\nexploder = TileExploder()\nTo “vectorize” the the band columns, we use the SparkML VectorAssembler. Each of the seven bands is a different feature.\nassembler = VectorAssembler() \\\n    .setInputCols(list(catalog_df.columns)) \\\n    .setOutputCol(\"features\")\nFor this problem, we will use the K-means clustering algorithm and configure our model to have 5 clusters.\nkmeans = KMeans().setK(5).setFeaturesCol('features')\nWe can combine the above stages into a single Pipeline.\npipeline = Pipeline().setStages([exploder, assembler, kmeans])","title":"Create ML Pipeline"},{"location":"/unsupervised-learning.html#fit-the-model-and-score","text":"Fitting the pipeline actually executes exploding the tiles, assembling the features vectors, and fitting the K-means clustering model.\nmodel = pipeline.fit(df)\nWe can use the transform function to score the training data in the fitted pipeline model. This will add a column called prediction with the closest cluster identifier.\nclustered = model.transform(df)\nNow let’s take a look at some sample output.\nclustered.select('prediction', 'extent', 'column_index', 'row_index', 'features')\nShowing only top 5 rows.\nprediction extent column_index row_index features 0 [703986.502389, 4249549.463864264, 709547.135536023, 4254601.8671] 0 0 [9470.0,8491.0,7805.0,6697.0,17507.0,10338.0,7235.0] 0 [703986.502389, 4249549.463864264, 709547.135536023, 4254601.8671] 1 0 [9566.0,8607.0,8046.0,6898.0,18504.0,11545.0,7877.0] 0 [703986.502389, 4249549.463864264, 709547.135536023, 4254601.8671] 2 0 [9703.0,8808.0,8377.0,7222.0,20556.0,13207.0,8686.0] 1 [703986.502389, 4249549.463864264, 709547.135536023, 4254601.8671] 3 0 [9856.0,8983.0,8565.0,7557.0,19479.0,13203.0,9065.0] 1 [703986.502389, 4249549.463864264, 709547.135536023, 4254601.8671] 4 0 [10105.0,9270.0,8851.0,7912.0,19074.0,12737.0,8947.0]\nIf we want to inspect the model statistics, the SparkML API requires us to go through this unfortunate contortion to access the clustering results:\ncluster_stage = model.stages[2]\nWe can then compute the sum of squared distances of points to their nearest center, which is elemental to most cluster quality metrics.\nmetric = cluster_stage.computeCost(clustered)\nprint(\"Within set sum of squared errors: %s\" % metric)\nWithin set sum of squared errors: 228028955354.5672","title":"Fit the Model and Score"},{"location":"/unsupervised-learning.html#visualize-prediction","text":"We can recreate the tiled data structure using the metadata added by the TileExploder pipeline stage.\nfrom pyrasterframes.rf_types import CellType\n\ntile_dims = df.select(rf_dimensions(df.b1).alias('dims')).first()['dims']\nretiled = clustered.groupBy('extent', 'crs') \\\n    .agg(\n        rf_assemble_tile('column_index', 'row_index', 'prediction',\n            tile_dims['cols'], tile_dims['rows'], CellType.int8()).alias('prediction')\n)\n\nretiled.printSchema()\nroot\n |-- extent: struct (nullable = true)\n |    |-- xmin: double (nullable = false)\n |    |-- ymin: double (nullable = false)\n |    |-- xmax: double (nullable = false)\n |    |-- ymax: double (nullable = false)\n |-- crs: struct (nullable = true)\n |    |-- crsProj4: string (nullable = false)\n |-- prediction: tile (nullable = true)\nretiled\nextent crs prediction [703986.502389, 4249549.463864264, 709547.135536023, 4254601.8671] [+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs ]\nThe resulting output is shown below.\ndisplay(retiled.select('prediction').first()['prediction'])","title":"Visualize Prediction"},{"location":"/supervised-learning.html","text":"","title":"Supervised Machine Learning"},{"location":"/supervised-learning.html#supervised-machine-learning","text":"In this example we will demonstrate how to fit and score a supervised learning model with a sample of Sentinel-2 data and hand-drawn vector labels over different land cover types.","title":"Supervised Machine Learning"},{"location":"/supervised-learning.html#create-and-read-raster-catalog","text":"The first step is to create a Spark DataFrame containing our imagery data. To achieve that we will create a catalog DataFrame. In the catalog, each row represents a distinct area and time, and each column is the URI to a band’s image product. In this example our catalog just has one row. After reading the catalog, the resulting Spark DataFrame may have many rows per URI, with a column corresponding to each band.\nThe imagery for feature data will come from eleven bands of 60 meter resolution Sentinel-2 imagery. We also will use the scene classification (SCL) data to identify high quality, non-cloudy pixels.\nuri_base = 's3://s22s-test-geotiffs/luray_snp/{}.tif'\nbands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12']\ncols = ['SCL'] + bands\n\ncatalog_df = pd.DataFrame([\n    {b: uri_base.format(b) for b in cols}\n])\n\ndf = spark.read.raster(catalog=catalog_df,\n\t\t\t\t\t   catalog_col_names=cols,\n\t\t\t\t\t   tile_dimensions=(128, 128)\n\t\t\t\t\t   ).repartition(100)\n\ndf = df.select(\n    rf_crs(df.B01).alias('crs'),\n    rf_extent(df.B01).alias('extent'),\n    rf_tile(df.SCL).alias('scl'),\n    rf_tile(df.B01).alias('B01'),\n    rf_tile(df.B02).alias('B02'),\n    rf_tile(df.B03).alias('B03'),\n    rf_tile(df.B04).alias('B04'),\n    rf_tile(df.B05).alias('B05'),\n    rf_tile(df.B06).alias('B06'),\n    rf_tile(df.B07).alias('B07'),\n    rf_tile(df.B08).alias('B08'),\n    rf_tile(df.B09).alias('B09'),\n    rf_tile(df.B11).alias('B11'),\n    rf_tile(df.B12).alias('B12'),\n)\ndf.printSchema()\nroot\n |-- crs: struct (nullable = true)\n |    |-- crsProj4: string (nullable = false)\n |-- extent: struct (nullable = true)\n |    |-- xmin: double (nullable = false)\n |    |-- ymin: double (nullable = false)\n |    |-- xmax: double (nullable = false)\n |    |-- ymax: double (nullable = false)\n |-- scl: tile (nullable = true)\n |-- B01: tile (nullable = true)\n |-- B02: tile (nullable = true)\n |-- B03: tile (nullable = true)\n |-- B04: tile (nullable = true)\n |-- B05: tile (nullable = true)\n |-- B06: tile (nullable = true)\n |-- B07: tile (nullable = true)\n |-- B08: tile (nullable = true)\n |-- B09: tile (nullable = true)\n |-- B11: tile (nullable = true)\n |-- B12: tile (nullable = true)","title":"Create and Read Raster Catalog"},{"location":"/supervised-learning.html#data-prep","text":"","title":"Data Prep"},{"location":"/supervised-learning.html#label-data","text":"The land classification labels are based on a small set of hand drawn polygons in the GeoJSON file here. The property id indicates the type of land cover in each area. For these integer values, 1 is forest, 2 is cropland, and 3 is developed areas.\nWe will create a very small Spark DataFrame of the label shapes and then join it to the raster DataFrame. Such joins are typically expensive, but in this case both datasets are quite small. To speed up the join for the small vector DataFrame, we put the broadcast hint on it, which will tell Spark to put a copy of it on each Spark executor.\nAfter the raster and vector data are joined, we will convert the vector shapes into tiles using the rf_rasterize function. This procedure is sometimes called “burning in” a geometry into a raster. The values in the resulting tile cells are the id property of the GeoJSON, which we will use as labels in our supervised learning task. In areas where the geometry does not intersect, the cells will contain NoData.\ncrses = df.select('crs.crsProj4').distinct().collect()\nprint('Found ', len(crses), 'distinct CRS.')\ncrs = crses[0][0]\n\nlabel_df = spark.read.geojson(\n    os.path.join(resource_dir_uri(), 'luray-labels.geojson')) \\\n    .select('id', st_reproject('geometry', lit('EPSG:4326'), lit(crs)).alias('geometry')) \\\n    .hint('broadcast')\n\ndf_joined = df.join(label_df, st_intersects(st_geometry('extent'), 'geometry')) \\\n    .withColumn('dims', rf_dimensions('B01'))\n\ndf_labeled = df_joined.withColumn('label', \n   rf_rasterize('geometry', st_geometry('extent'), 'id', 'dims.cols', 'dims.rows')\n)\nFound  1 distinct CRS.","title":"Label Data"},{"location":"/supervised-learning.html#masking-poor-quality-cells","text":"To filter only for good quality pixels, we follow roughly the same procedure as demonstrated in the quality masking section of the chapter on NoData. Instead of actually setting NoData values in the unwanted cells of any of the imagery bands, we will just filter out the mask cell values later in the process.\nfrom pyspark.sql.functions import lit\n\nmask_part = df_labeled \\\n    .withColumn('nodata', rf_local_equal('scl', lit(0))) \\\n    .withColumn('defect', rf_local_equal('scl', lit(1))) \\\n    .withColumn('cloud8', rf_local_equal('scl', lit(8))) \\\n    .withColumn('cloud9', rf_local_equal('scl', lit(9))) \\\n    .withColumn('cirrus', rf_local_equal('scl', lit(10)))\n\ndf_mask_inv = mask_part \\\n    .withColumn('mask', rf_local_add('nodata', 'defect')) \\\n    .withColumn('mask', rf_local_add('mask', 'cloud8')) \\\n    .withColumn('mask', rf_local_add('mask', 'cloud9')) \\\n    .withColumn('mask', rf_local_add('mask', 'cirrus')) \\\n    .drop('nodata', 'defect', 'cloud8', 'cloud9', 'cirrus')\n    \n# at this point the mask contains 0 for good cells and 1 for defect, etc\n# convert cell type and set value 1 to NoData\ndf_mask = df_mask_inv.withColumn('mask',\n  rf_with_no_data(rf_convert_cell_type('mask', 'uint8'), 1.0)\n)\n\ndf_mask.printSchema()\nroot\n |-- crs: struct (nullable = true)\n |    |-- crsProj4: string (nullable = false)\n |-- extent: struct (nullable = true)\n |    |-- xmin: double (nullable = false)\n |    |-- ymin: double (nullable = false)\n |    |-- xmax: double (nullable = false)\n |    |-- ymax: double (nullable = false)\n |-- scl: tile (nullable = true)\n |-- B01: tile (nullable = true)\n |-- B02: tile (nullable = true)\n |-- B03: tile (nullable = true)\n |-- B04: tile (nullable = true)\n |-- B05: tile (nullable = true)\n |-- B06: tile (nullable = true)\n |-- B07: tile (nullable = true)\n |-- B08: tile (nullable = true)\n |-- B09: tile (nullable = true)\n |-- B11: tile (nullable = true)\n |-- B12: tile (nullable = true)\n |-- id: long (nullable = true)\n |-- geometry: geometry (nullable = true)\n |-- dims: struct (nullable = true)\n |    |-- cols: short (nullable = false)\n |    |-- rows: short (nullable = false)\n |-- label: tile (nullable = true)\n |-- mask: tile (nullable = true)","title":"Masking Poor Quality Cells"},{"location":"/supervised-learning.html#create-ml-pipeline","text":"We import various Spark components that we need to construct our Pipeline. These are the objects that will work in sequence to conduct the data preparation and modeling.\nfrom pyrasterframes import TileExploder\nfrom pyrasterframes.rf_types import NoDataFilter\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\nSparkML requires that each observation be in its own row, and those observations be packed into a single Vector object. The first step is to “explode” the tiles into a single row per cell or pixel with the TileExploder (see also rf_explode_tiles). If a tile cell contains a NoData it will become a null value after the exploder stage. Then we use the NoDataFilter to filter out any rows that missing or null values, which will cause an error during training. Finally we use the SparkML VectorAssembler to create that Vector.\nRecall above we set undesirable pixels to NoData, so the NoDataFilter will remove them at this stage. We apply the filter to the mask column and the label column, the latter being used during training. When it is time to score the model, the pipeline will ignore the fact that there is no label column on the input DataFrame.\nexploder = TileExploder()\n\nnoDataFilter = NoDataFilter() \\\n  .setInputCols(['label', 'mask'])\n\nassembler = VectorAssembler() \\\n  .setInputCols(bands) \\\n  .setOutputCol(\"features\")\nWe are going to use a decision tree for classification. You can swap out one of the other multi-class classification algorithms if you like. With the algorithm selected we can assemble our modeling pipeline.\nclassifier = DecisionTreeClassifier() \\\n  .setLabelCol('label') \\\n  .setFeaturesCol(assembler.getOutputCol())\n\npipeline = Pipeline() \\\n  .setStages([exploder, noDataFilter, assembler, classifier])\n\npipeline.getStages()\n[TileExploder_4a6b9d5a4f1631e9a348,\n NoDataFilter_40f386af8051d2324bae,\n VectorAssembler_4aae9d5214c22486390d,\n DecisionTreeClassifier_4b03bba297b9dc30c59c]","title":"Create ML Pipeline"},{"location":"/supervised-learning.html#train-the-model","text":"The next step is to actually run each step of the Pipeline we created, including fitting the decision tree model. We filter the DataFrame for only tiles intersecting the label raster because the label shapes are relatively sparse over the imagery. It would be logically equivalent to either include or exclude thi step, but it is more efficient to filter because it will mean less data going into the pipeline.\nmodel_input = df_mask.filter(rf_tile_sum('label') > 0).cache()\nmodel = pipeline.fit(model_input)","title":"Train the Model"},{"location":"/supervised-learning.html#model-evaluation","text":"To view the model’s performance, we first call the pipeline’s transform method on the training dataset. This transformed dataset will have the model’s prediction included in each row. We next construct an evaluator and pass it the transformed dataset to easily compute the performance metric. We can also create custom metrics using a variety of DataFrame or SQL transformations.\nprediction_df = model.transform(df_mask) \\\n                       .drop(assembler.getOutputCol()).cache()\nprediction_df.printSchema()\n\neval = MulticlassClassificationEvaluator(\n    predictionCol=classifier.getPredictionCol(),\n    labelCol=classifier.getLabelCol(),\n    metricName='accuracy'\n)\n\naccuracy = eval.evaluate(prediction_df)\nprint(\"\\nAccuracy:\", accuracy)\nroot\n |-- crs: struct (nullable = true)\n |    |-- crsProj4: string (nullable = false)\n |-- extent: struct (nullable = true)\n |    |-- xmin: double (nullable = false)\n |    |-- ymin: double (nullable = false)\n |    |-- xmax: double (nullable = false)\n |    |-- ymax: double (nullable = false)\n |-- id: long (nullable = true)\n |-- geometry: geometry (nullable = true)\n |-- dims: struct (nullable = true)\n |    |-- cols: short (nullable = false)\n |    |-- rows: short (nullable = false)\n |-- column_index: integer (nullable = false)\n |-- row_index: integer (nullable = false)\n |-- scl: double (nullable = false)\n |-- B01: double (nullable = false)\n |-- B02: double (nullable = false)\n |-- B03: double (nullable = false)\n |-- B04: double (nullable = false)\n |-- B05: double (nullable = false)\n |-- B06: double (nullable = false)\n |-- B07: double (nullable = false)\n |-- B08: double (nullable = false)\n |-- B09: double (nullable = false)\n |-- B11: double (nullable = false)\n |-- B12: double (nullable = false)\n |-- label: double (nullable = false)\n |-- mask: double (nullable = false)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n\nAccuracy: 0.9734062761188359\nAs an example of using the flexibility provided by DataFrames, the code below computes and displays the confusion matrix.\ncnf_mtrx = prediction_df.groupBy(classifier.getPredictionCol()) \\\n    .pivot(classifier.getLabelCol()) \\\n    .count() \\\n    .sort(classifier.getPredictionCol())\ncnf_mtrx\nprediction 1.0 2.0 3.0 1.0 6603 9 53 2.0 10 1739 48 3.0 54 176 4469","title":"Model Evaluation"},{"location":"/supervised-learning.html#visualize-prediction","text":"Because the pipeline included a TileExploder, we will recreate the tiled data structure. The explosion transformation includes metadata enabling us to recreate the tiles. See the rf_assemble_tile function documentation for more details. In this case, the pipeline is scoring on all areas, regardless of whether they intersect the label polygons. This is simply done by removing the label column, as discussed above.\nscored = model.transform(df_mask.drop('label'))\n\nretiled = scored \\\n    .groupBy('extent', 'crs') \\\n    .agg(\n        rf_assemble_tile('column_index', 'row_index', 'prediction', 128, 128).alias('prediction'),\n        rf_assemble_tile('column_index', 'row_index', 'B04', 128, 128).alias('red'),\n        rf_assemble_tile('column_index', 'row_index', 'B03', 128, 128).alias('grn'),\n        rf_assemble_tile('column_index', 'row_index', 'B02', 128, 128).alias('blu')\n    )\nretiled.printSchema()\nroot\n |-- extent: struct (nullable = true)\n |    |-- xmin: double (nullable = false)\n |    |-- ymin: double (nullable = false)\n |    |-- xmax: double (nullable = false)\n |    |-- ymax: double (nullable = false)\n |-- crs: struct (nullable = true)\n |    |-- crsProj4: string (nullable = false)\n |-- prediction: tile (nullable = true)\n |-- red: tile (nullable = true)\n |-- grn: tile (nullable = true)\n |-- blu: tile (nullable = true)\nTake a look at a sample of the resulting output and the corresponding area’s red-green-blue composite image.\nsample = retiled \\\n    .select('prediction', rf_rgb_composite('red', 'grn', 'blu').alias('rgb')) \\\n    .sort(-rf_tile_sum(rf_local_equal('prediction', lit(3.0)))) \\\n    .first()\n\nsample_rgb = sample['rgb']\nmins = np.nanmin(sample_rgb.cells, axis=(0,1))\nplt.imshow((sample_rgb.cells -  mins) / (np.nanmax(sample_rgb.cells, axis=(0,1)) - mins))\nsetup.py:5: RuntimeWarning: overflow encountered in int_scalars\n  #\n<matplotlib.image.AxesImage at 0x11d524080>\nRecall the label coding: 1 is forest (purple), 2 is cropland (green) and 3 is developed areas(yellow).\ndisplay(sample['prediction'])\n/anaconda3/lib/python3.6/site-packages/numpy/ma/core.py:2569: RuntimeWarning: invalid value encountered in minimum\n  result = getattr(self._data, funcname)(*args, **params)\n/anaconda3/lib/python3.6/site-packages/numpy/ma/core.py:2569: RuntimeWarning: invalid value encountered in maximum\n  result = getattr(self._data, funcname)(*args, **params)","title":"Visualize Prediction"},{"location":"/numpy-pandas.html","text":"","title":"NumPy and Pandas"},{"location":"/numpy-pandas.html#numpy-and-pandas","text":"In the Python Spark API, the work of distributed computing over the DataFrame is done on many executors (the Spark term for workers) inside Java virtual machines (JVM). Most calls to pyspark are passed to a Java process via the py4j library. The user can also ask for data inside the JVM to be brought over to the Python driver (the Spark term for the client application). When dealing with tiles, the driver will receive this data as a lightweight wrapper object around a NumPy ndarray. It is also possible to write lambda functions against NumPy arrays and evaluate them in the Spark DataFrame.","title":"NumPy and Pandas"},{"location":"/numpy-pandas.html#performance-considerations","text":"When working with large, distributed datasets in Spark, attention is required when invoking actions on the data. In general, transformations are lazily evaluated in Spark, meaning the code runs fast and it doesn’t move any data around. But actions cause the evaluation to happen, meaning all the lazily planned transformations are going to be computed and data is going to be processed and moved around. In general, if a pyspark function returns a DataFrame, it is probably a transformation, and if not, it is an action.\nWhen many actions are invoked, a lot of data can flow from executors to the driver. In pyspark, the data then has to move from the driver JVM to the Python process running the driver. When that happens, if there are any tiles in the data, they will be converted to a Python Tile object. In practical work with Earth observation data, the tiles are frequently 256 by 256 arrays, which may be 100kb or more each. Individually they are small, but a DataFrame can easily have dozens of such tile columns and millions of rows.\nAll of this discussion reinforces two important principles for working with Spark: understanding the cost of an action and using aggreates, summaries, or samples to manage the cost of actions.","title":"Performance Considerations"},{"location":"/numpy-pandas.html#the-tile-class","text":"In Python, tiles are represented with the rf_types.Tile class. This is a NumPy ndarray with two dimensions, along with some additional metadata allowing correct conversion to the GeoTrellis cell type.\nfrom pyrasterframes.rf_types import Tile\nimport numpy as np\n\nt = Tile(np.random.randn(4, 4))\nprint(str(t))\nTile(dimensions=[4, 4], cell_type=CellType(float64, nan), cells=\n[[-0.5863218958595482 1.795796341124276 -2.0852535607684466\n  0.19724604210206995]\n [-2.382534178654329 -1.5061790064376177 -1.4530778617968478\n  0.4851696050465145]\n [0.9614034817145716 1.567105484287035 -1.441862575234524\n  -1.558194231034949]\n [1.406906959001929 0.017706894129644584 0.34752015804954844\n  0.16487613693126169]])\nYou can access the NumPy array with the cells member of Tile.\nt.cells.shape, t.cells.nbytes\n((4, 4), 128)","title":"The Tile Class"},{"location":"/numpy-pandas.html#dataframe-topandas","text":"As discussed in the raster writing chapter, a pretty display of Pandas DataFrame containing tiles is available by importing the rf_ipython submodule. In addition, as discussed in the vector data chapter, any geometry type in the Spark DataFrame will be converted into a Shapely geometry. Taken together, we can easily get the spatial information and raster data as a NumPy array, all within a Pandas DataFrame.\nimport pyrasterframes.rf_ipython\nfrom pyspark.sql.functions import lit, col\n\ncat = spark.read.format('aws-pds-modis-catalog').load() \\\n        .filter(\n            (col('granule_id') == 'h11v04') &\n            (col('acquisition_date') > lit('2018-02-19')) &\n            (col('acquisition_date') < lit('2018-02-22'))\n        )\n\nspark_df = spark.read.raster(catalog=cat, catalog_col_names=['B01']) \\\n                .select(\n                    'acquisition_date',\n                    'granule_id',\n                    rf_tile('B01').alias('tile'),\n                    rf_geometry('B01').alias('tile_geom')\n                    )\n\npandas_df = spark_df.limit(10).toPandas()\npandas_df.iloc[0].apply(lambda v: type(v))\nacquisition_date    <class 'pandas._libs.tslibs.timestamps.Timesta...\ngranule_id                                              <class 'str'>\ntile                           <class 'pyrasterframes.rf_types.Tile'>\ntile_geom                  <class 'shapely.geometry.polygon.Polygon'>\nName: 0, dtype: object","title":"DataFrame toPandas"},{"location":"/numpy-pandas.html#user-defined-functions","text":"As we demonstrated with vector data, we can also make use of the Tile type to create user-defined functions (UDF) that can take a tile as input, return a tile as output, or both. Here is a trivial and inefficient example of doing both. A serious performance implication of user defined functions in Python is that all the executors must move the Java objects to Python, evaluate the function, and then move the Python objects back to Java. Use the many built-in functions wherever possible, and ask the community if you have an idea for a function that should be included.\nWe will demonstrate an example of creating a UDF that is logically equivalent to a built-in function. We’ll quickly show that the resulting tiles are approximately equivalent. The reason they are not exactly the same is that one is computed in Python and the other is computed in Java.\nfrom pyrasterframes.rf_types import TileUDT\nfrom pyspark.sql.functions import udf\n\n@udf(TileUDT())\ndef my_udf(t):\n    import numpy as np\n    return Tile(np.log1p(t.cells))\n\nudf_df = spark_df.limit(1).select(\n            my_udf('tile').alias('udf_result'),\n            rf_log1p('tile').alias('built_in_result')\n        ).toPandas()\n\nrow = udf_df.iloc[0]\ndiff = row['udf_result'] - row['built_in_result']\nprint(type(diff))\nnp.abs(diff.cells).max()\n<class 'pyrasterframes.rf_types.Tile'>\n4.764826915248932e-07\nWe can also inspect an image of the difference between the two tiles, which is just random noise. Both tiles have the same structure of NoData, as exhibited by the white areas.\ndiff.show(0, 100)\n<matplotlib.axes._subplots.AxesSubplot at 0x129c6d6d8>","title":"User Defined Functions"},{"location":"/numpy-pandas.html#creating-a-spark-dataframe","text":"You can also create a Spark DataFrame with a column full of Tile objects or Shapely geomtery objects.\nThe example below will create a Pandas DataFrame with ten rows of noise tiles and random Points. We will then create a Spark DataFrame from it.\nimport pandas as pd\nfrom shapely.geometry import Point\n\npandas_df = pd.DataFrame([{\n    'tile': Tile(np.random.randn(100, 100)),\n    'geom': Point(-90 + 90 * np.random.random((2, 1)))\n    } for _ in range(10)\n])\n\nspark_df = spark.createDataFrame(pandas_df)\n\nspark_df.printSchema()\nspark_df.count()\nroot\n |-- tile: tile (nullable = true)\n |-- geom: point (nullable = true)\n10","title":"Creating a Spark DataFrame"},{"location":"/languages.html","text":"","title":"API Languages"},{"location":"/languages.html#api-languages","text":"One of the great powers of RasterFrames, afforded by Spark SQL, is the ability to express computation in multiple programming languages. This documentation focuses on Python because it is the most commonly used language in data science and GIS analytics. However, Scala (the implementation language of RasterFrames) and SQL are also fully supported. Examples in Python can be mechanically translated into the other two languages without much difficulty once the naming conventions are understood. In the sections below we will show the same example program in each language. We will compute the average NDVI per month for a single tile in Tanzania.","title":"API Languages"},{"location":"/languages.html#python","text":"","title":"Python"},{"location":"/languages.html#step-1-load-the-catalog","text":"modis = spark.read.format('aws-pds-modis-catalog').load()","title":"Step 1: Load the catalog"},{"location":"/languages.html#step-2-down-select-data-by-month","text":"red_nir_monthly_2017 = modis \\\n    .select(\n        col('granule_id'),\n        month('acquisition_date').alias('month'),\n        col('B01').alias('red'),\n        col('B02').alias('nir')\n    ) \\\n    .where(\n        (year('acquisition_date') == 2017) & \n        (dayofmonth('acquisition_date') == 15) & \n        (col('granule_id') == 'h21v09')\n    )\nred_nir_monthly_2017.printSchema()\nroot\n |-- granule_id: string (nullable = false)\n |-- month: integer (nullable = false)\n |-- red: string (nullable = true)\n |-- nir: string (nullable = true)","title":"Step 2: Down-select data by month"},{"location":"/languages.html#step-3-read-tiles","text":"red_nir_tiles_monthly_2017 = spark.read.raster(\n    catalog=red_nir_monthly_2017,\n    catalog_col_names=['red', 'nir'],\n    tile_dimensions=(256, 256)\n)","title":"Step 3: Read tiles"},{"location":"/languages.html#step-4-compute-aggregates","text":"result = red_nir_tiles_monthly_2017 \\\n    .where(st_intersects(\n        st_reproject(rf_geometry(col('red')), rf_crs(col('red')).crsProj4, rf_mk_crs('EPSG:4326')),\n        st_makePoint(lit(34.870605), lit(-4.729727)))\n    ) \\\n    .groupBy('month') \\\n    .agg(rf_agg_stats(rf_normalized_difference(col('nir'), col('red'))).alias('ndvi_stats')) \\\n    .orderBy(col('month')) \\\n    .select('month', 'ndvi_stats.*')\nresult\nShowing only top 5 rows.\nmonth data_cells no_data_cells min max mean variance 1 65523 13 -0.2519809825673534 0.8644836272040303 0.4062596673810191 0.01407280838385605 2 65521 15 -0.21232123212321233 0.8872390789756832 0.46738863804011127 0.011822698212885174 3 64425 1111 -0.36211340206185566 0.9208261617900172 0.5811071411891395 0.011570245465885753 4 64236 1300 -0.17252657399836469 0.922397476340694 0.5254596885897274 0.01087259231886667 5 60819 4717 -0.19951338199513383 0.916626036079961 0.46471430090039234 0.01215478443067744","title":"Step 4: Compute aggregates"},{"location":"/languages.html#sql","text":"For convenience, we’re going to evaluate SQL from the Python environment. The SQL fragments should work in the spark-sql shell just the same.\ndef sql(stmt):\n    return spark.sql(stmt)","title":"SQL"},{"location":"/languages.html#step-1-load-the-catalog","text":"sql(\"CREATE OR REPLACE TEMPORARY VIEW modis USING `aws-pds-modis-catalog`\")","title":"Step 1: Load the catalog"},{"location":"/languages.html#step-2-down-select-data-by-month","text":"sql(\"\"\"\nCREATE OR REPLACE TEMPORARY VIEW red_nir_monthly_2017 AS\nSELECT granule_id, month(acquisition_date) as month, B01 as red, B02 as nir\nFROM modis\nWHERE year(acquisition_date) = 2017 AND day(acquisition_date) = 15 AND granule_id = 'h21v09'\n\"\"\")\nsql('DESCRIBE red_nir_monthly_2017')\ncol_name data_type comment granule_id string month int red string nir string","title":"Step 2: Down-select data by month"},{"location":"/languages.html#step-3-read-tiles","text":"sql(\"\"\"\nCREATE OR REPLACE TEMPORARY VIEW red_nir_tiles_monthly_2017\nUSING raster\nOPTIONS (\n    catalogTable='red_nir_monthly_2017',\n    catalogColumns='red,nir',\n    tileDimensions='256,256'\n    )\n\"\"\")","title":"Step 3: Read tiles"},{"location":"/languages.html#step-4-compute-aggregates","text":"grouped = sql(\"\"\"\nSELECT month, ndvi_stats.* FROM (\n    SELECT month, rf_agg_stats(rf_normalized_difference(nir, red)) as ndvi_stats\n    FROM red_nir_tiles_monthly_2017\n    WHERE st_intersects(st_reproject(rf_geometry(red), rf_crs(red), 'EPSG:4326'), st_makePoint(34.870605, -4.729727))\n    GROUP BY month\n    ORDER BY month\n)\n\"\"\")\ngrouped\nShowing only top 5 rows.\nmonth data_cells no_data_cells min max mean variance 1 65523 13 -0.2519809825673534 0.8644836272040303 0.4062596673810191 0.01407280838385605 2 65521 15 -0.21232123212321233 0.8872390789756832 0.46738863804011127 0.011822698212885174 3 64425 1111 -0.36211340206185566 0.9208261617900172 0.5811071411891395 0.011570245465885753 4 64236 1300 -0.17252657399836469 0.922397476340694 0.5254596885897274 0.01087259231886667 5 60819 4717 -0.19951338199513383 0.916626036079961 0.46471430090039234 0.01215478443067744","title":"Step 4: Compute aggregates"},{"location":"/languages.html#scala","text":"The latest Scala API documentation is available here:\nScala API Documentation","title":"Scala"},{"location":"/languages.html#step-1-load-the-catalog","text":"import geotrellis.proj4.LatLng\nimport org.locationtech.rasterframes._\nimport org.locationtech.rasterframes.datasource.raster._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\n\n\nimplicit val spark = SparkSession.builder()\n  .master(\"local[*]\")\n  .appName(\"RasterFrames\")\n  .withKryoSerialization\n  .getOrCreate()\n  .withRasterFrames\n\nimport spark.implicits._\n\nval modis = spark.read.format(\"aws-pds-modis-catalog\").load()","title":"Step 1: Load the catalog"},{"location":"/languages.html#step-2-down-select-data-by-month","text":"val red_nir_monthly_2017 = modis\n  .select($\"granule_id\", month($\"acquisition_date\") as \"month\", $\"B01\" as \"red\", $\"B02\" as \"nir\")\n  .where(year($\"acquisition_date\") === 2017 && (dayofmonth($\"acquisition_date\") === 15) && $\"granule_id\" === \"h21v09\")","title":"Step 2: Down-select data by month"},{"location":"/languages.html#step-3-read-tiles","text":"val red_nir_tiles_monthly_2017 = spark.read.raster\n  .fromCatalog(red_nir_monthly_2017, \"red\", \"nir\")\n  .load()","title":"Step 3: Read tiles"},{"location":"/languages.html#step-4-compute-aggregates","text":"val result = red_nir_tiles_monthly_2017\n  .where(st_intersects(\n    st_reproject(rf_geometry($\"red\"), rf_crs($\"red\"), LatLng),\n    st_makePoint(34.870605, -4.729727)\n  ))\n  .groupBy(\"month\")\n  .agg(rf_agg_stats(rf_normalized_difference($\"nir\", $\"red\")) as \"ndvi_stats\")\n  .orderBy(\"month\")\n  .select(\"month\", \"ndvi_stats.*\")\n  \nresult.show()","title":"Step 4: Compute aggregates"},{"location":"/reference.html","text":"","title":"Function Reference"},{"location":"/reference.html#function-reference","text":"RasterFrames provides a rich set of columnar function for processing geospatial raster data. In Spark SQL, the functions are already registered in the SQL engine; they are usually prefixed with rf_. In Python, they are available in the pyrasterframes.rasterfunctions module.\nThe convention in this document will be to define the function signature as below, with its return type, the function name, and named arguments with their types.\nReturnDataType function_name(InputDataType argument1, InputDataType argument2)\nFor the Scala documentation on these functions, see RasterFunctions. The full Scala API documentation can be found here.","title":"Function Reference"},{"location":"/reference.html#list-of-available-sql-and-python-functions","text":"Vector Operations st_reproject st_extent st_geometry Tile Metadata and Mutation rf_dimensions rf_cell_type rf_tile rf_extent rf_crs rf_mk_crs rf_convert_cell_type rf_interpret_cell_type_as rf_resample Tile Creation rf_make_zeros_tile rf_make_ones_tile rf_make_constant_tile rf_rasterize rf_array_to_tile rf_assemble_tile Masking and NoData rf_mask rf_inverse_mask rf_mask_by_value rf_is_no_data_tile rf_local_no_data rf_local_data rf_local_data rf_with_no_data Local Map Algebra rf_local_add rf_local_subtract rf_local_multiply rf_local_divide rf_normalized_difference rf_local_less rf_local_less_equal rf_local_greater rf_local_greater_equal rf_local_equal rf_local_unequal rf_round rf_abs rf_exp rf_exp10 rf_exp2 rf_expm1 rf_log rf_log10 rf_log2 rf_log1p Tile Statistics rf_tile_sum rf_tile_mean rf_tile_min rf_tile_max rf_no_data_cells rf_data_cells rf_exists rf_for_all rf_tile_stats rf_tile_histogram Aggregate Tile Statistics rf_agg_mean rf_agg_data_cells rf_agg_no_data_cells rf_agg_stats rf_agg_approx_histogram Tile Local Aggregate Statistics rf_agg_local_max rf_agg_local_min rf_agg_local_mean rf_agg_local_data_cells rf_agg_local_no_data_cells rf_agg_local_stats Converting Tiles rf_explode_tiles rf_explode_tiles_sample rf_tile_to_array_int rf_tile_to_array_double rf_render_ascii rf_render_matrix rf_rgb_composite rf_render_png\nTo import RasterFrames functions into the environment, import from pyrasterframes.rasterfunctions.\nfrom pyrasterframes.rasterfunctions import *\nFunctions starting with rf_, which are for raster, and st_, which are for vector geometry, become available for use with DataFrames. You can view all of the available functions with the following.\n[fn for fn in dir() if fn.startswith('rf_') or fn.startswith('st_')]","title":"List of Available SQL and Python Functions"},{"location":"/reference.html#vector-operations","text":"Various LocationTech GeoMesa user-defined functions (UDFs) dealing with geomtery type columns are provided in the SQL engine and within the pyrasterframes.rasterfunctions Python module. These are documented in the LocationTech GeoMesa Spark SQL documentation. These functions are all prefixed with st_.\nRasterFrames provides some additional functions for vector geometry operations.","title":"Vector Operations"},{"location":"/reference.html#st-reproject","text":"Geometry st_reproject(Geometry geom, String origin_crs, String destination_crs)\nReproject the vector geom from origin_crs to destination_crs. Both _crs arguments are either proj4 strings, EPSG codes or OGC WKT for coordinate reference systems.","title":"st_reproject"},{"location":"/reference.html#st-extent","text":"Struct[Double xmin, Double xmax, Double ymin, Double ymax] st_extent(Geometry geom)\nExtracts the bounding box (extent/envelope) of the geometry.\nSee also GeoMesa st_envelope which returns a Geometry type.","title":"st_extent"},{"location":"/reference.html#st-geometry","text":"Geometry st_geometry(Struct[Double xmin, Double xmax, Double ymin, Double ymax] extent)\nConvert an extent to a Geometry. The extent likely comes from st_extent or rf_extent.","title":"st_geometry"},{"location":"/reference.html#tile-metadata-and-mutation","text":"Functions to access and change the particulars of a tile: its shape and the data type of its cells. See section on “NoData” handling for additional discussion of cell types.","title":"Tile Metadata and Mutation"},{"location":"/reference.html#rf-dimensions","text":"Struct[Int, Int] rf_dimensions(Tile tile)\nGet number of columns and rows in the tile, as a Struct of cols and rows.","title":"rf_dimensions"},{"location":"/reference.html#rf-cell-type","text":"Struct[String] rf_cell_type(Tile tile)\nGet the cell type of the tile. The cell type can be changed with rf_convert_cell_type.","title":"rf_cell_type"},{"location":"/reference.html#rf-tile","text":"Tile rf_tile(ProjectedRasterTile proj_raster)\nGet the fully realized (non-lazy) tile from a ProjectedRasterTile struct column.","title":"rf_tile"},{"location":"/reference.html#rf-extent","text":"Struct[Double xmin, Double xmax, Double ymin, Double ymax] rf_extent(ProjectedRasterTile proj_raster)\nStruct[Double xmin, Double xmax, Double ymin, Double ymax] rf_extent(RasterSource proj_raster)\nFetches the extent (bounding box or envelope) of a ProjectedRasterTile or RasterSource type tile columns.","title":"rf_extent"},{"location":"/reference.html#rf-crs","text":"Struct rf_crs(ProjectedRasterTile proj_raster)\nStruct rf_crs(RasterSource proj_raster)\nFetch CRS structure representing the coordinate reference system of a ProjectedRasterTile or RasterSource type tile columns.","title":"rf_crs"},{"location":"/reference.html#rf-mk-crs","text":"Struct rf_mk_crs(String crsText)\nConstruct a CRS structure from one of its string representations. Three forms are supported:\nEPSG code: EPSG:<integer> Proj4 string: +proj <proj4 parameters> WKT String with embedded EPSG code: GEOGCS[\"<name>\", <datum>, <prime meridian>, <angular unit> {,<twin axes>} {,<authority>}]\nExample: SELECT rf_mk_crs('EPSG:4326')","title":"rf_mk_crs"},{"location":"/reference.html#rf-convert-cell-type","text":"Tile rf_convert_cell_type(Tile tile_col, CellType cell_type)\nTile rf_convert_cell_type(Tile tile_col, String cell_type)\nConvert tile_col to a different cell type. In Python you can pass a CellType object to cell_type.","title":"rf_convert_cell_type"},{"location":"/reference.html#rf-interpret-cell-type-as","text":"Tile rf_interpret_cell_type_as(Tile tile_col, CellType cell_type)\nTile rf_interpret_cell_type_as(Tile tile_col, String cell_type)\nChange the interpretation of the tile_col’s cell values according to specified cell_type. In Python you can pass a CellType object to cell_type.","title":"rf_interpret_cell_type_as"},{"location":"/reference.html#rf-resample","text":"Tile rf_resample(Tile tile, Double factor)\nTile rf_resample(Tile tile, Int factor)\nTile rf_resample(Tile tile, Tile shape_tile)\nChange the tile dimension. Passing a numeric factor will scale the number of columns and rows in the tile: 1.0 is the same number of columns and row; less than one downsamples the tile; and greater than one upsamples the tile. Passing a shape_tile as the second argument outputs tile having the same number of columns and rows as shape_tile. All resampling is by nearest neighbor method.","title":"rf_resample"},{"location":"/reference.html#tile-creation","text":"Functions to create a new Tile column, either from scratch or from existing data not yet in a tile.","title":"Tile Creation"},{"location":"/reference.html#rf-make-zeros-tile","text":"Tile rf_make_zeros_tile(Int tile_columns, Int tile_rows, [CellType cell_type])\nTile rf_make_zeros_tile(Int tile_columns, Int tile_rows, [String cell_type_name])\nCreate a tile of shape tile_columns by tile_rows full of zeros, with the optional cell type; default is float64. See this discussion on cell types for info on the cell_type argument. All arguments are literal values and not column expressions.","title":"rf_make_zeros_tile"},{"location":"/reference.html#rf-make-ones-tile","text":"Tile rf_make_ones_tile(Int tile_columns, Int tile_rows, [CellType cell_type])\nTile rf_make_ones_tile(Int tile_columns, Int tile_rows, [String cell_type_name])\nCreate a tile of shape tile_columns by tile_rows full of ones, with the optional cell type; default is float64. See this discussion on cell types for info on the cell_type argument. All arguments are literal values and not column expressions.","title":"rf_make_ones_tile"},{"location":"/reference.html#rf-make-constant-tile","text":"Tile rf_make_constant_tile(Numeric constant, Int tile_columns, Int tile_rows,  [CellType cell_type])\nTile rf_make_constant_tile(Numeric constant, Int tile_columns, Int tile_rows,  [String cell_type_name])\nCreate a tile of shape tile_columns by tile_rows full of constant, with the optional cell type; default is float64. See this discussion on cell types for info on the cell_type argument. All arguments are literal values and not column expressions.","title":"rf_make_constant_tile"},{"location":"/reference.html#rf-rasterize","text":"Tile rf_rasterize(Geometry geom, Geometry tile_bounds, Int value, Int tile_columns, Int tile_rows)\nConvert a vector Geometry geom into a Tile representation. The value will be “burned-in” to the returned tile where the geom intersects the tile_bounds. Returned tile will have shape tile_columns by tile_rows. Values outside the geom will be assigned a NoData value. Returned tile has cell type int32, note that value is of type Int.\nParameters tile_columns and tile_rows are literals, not column expressions. The others are column expressions.","title":"rf_rasterize"},{"location":"/reference.html#rf-array-to-tile","text":"Tile rf_array_to_tile(Array arrayCol, Int numCols, Int numRows)\nPython only. Create a tile from a Spark SQL Array, filling values in row-major order.","title":"rf_array_to_tile"},{"location":"/reference.html#rf-assemble-tile","text":"Tile rf_assemble_tile(Int colIndex, Int rowIndex, Numeric cellData, Int numCols, Int numRows, [CellType cell_type])\nTile rf_assemble_tile(Int colIndex, Int rowIndex, Numeric cellData, Int numCols, Int numRows, [String cell_type_name])\nSQL: Tile rf_assemble_tile(Int colIndex, Int rowIndex, Numeric cellData, Int numCols, Int numRows)\nCreate tiles of dimension numCols by numRows from a column of cell data with location indices. This function is the inverse of rf_explode_tiles. Intended use is with a groupby, producing one row with a new tile per group. In Python, the numCols, numRows and cellType arguments are literal values, others are column expressions. See this discussion on cell types for info on the optional cell_type argument. The default is float64. SQL implementation does not accept a cell_type argument. It returns a float64 cell type tile by default.","title":"rf_assemble_tile"},{"location":"/reference.html#masking-and-nodata","text":"See NoData handling for conceptual discussion of cell types and NoData.\nThere are statistical functions of the count of data and NoData values per tile and aggregate over a tile column: rf_data_cells, rf_no_data_cells, rf_agg_data_cells, and rf_agg_no_data_cells.\nMasking is a raster operation that sets specific cells to NoData based on the values in another raster.","title":"Masking and NoData"},{"location":"/reference.html#rf-mask","text":"Tile rf_mask(Tile tile, Tile mask)\nWhere the mask contains NoData, replace values in the tile with NoData.\nReturned tile cell type will be coerced to one supporting NoData if it does not already.\nSee also @rf_rasterize.","title":"rf_mask"},{"location":"/reference.html#rf-inverse-mask","text":"Tile rf_inverse_mask(Tile tile, Tile mask)\nWhere the mask does not contain NoData, replace values in tile with NoData.","title":"rf_inverse_mask"},{"location":"/reference.html#rf-mask-by-value","text":"Tile rf_mask_by_value(Tile data_tile, Tile mask_tile, Int mask_value)\nGenerate a tile with the values from data_tile, with NoData in cells where the mask_tile is equal to mask_value.","title":"rf_mask_by_value"},{"location":"/reference.html#rf-is-no-data-tile","text":"Boolean rf_is_no_data_tile(Tile)\nReturns true if tile contains only NoData. By definition returns false if cell type does not support NoData. To count NoData cells or data cells, see rf_no_data_cells, rf_data_cells, rf_agg_no_data_cells, rf_agg_data_cells, rf_agg_local_no_data_cells, and rf_agg_local_data_cells. This function is distinguished from rf_for_all, which tests that values are not NoData and nonzero.","title":"rf_is_no_data_tile"},{"location":"/reference.html#rf-local-no-data","text":"Tile rf_local_no_data(Tile tile)\nReturns a tile with values of 1 in each cell where the input tile contains NoData. Otherwise values are 0.","title":"rf_local_no_data"},{"location":"/reference.html#rf-local-data","text":"Tile rf_local_no_data(Tile tile)\nReturns a tile with values of 0 in each cell where the input tile contains NoData. Otherwise values are 1.","title":"rf_local_data"},{"location":"/reference.html#rf-local-data","text":"","title":"rf_local_data"},{"location":"/reference.html#rf-with-no-data","text":"Tile rf_with_no_data(Tile tile, Double no_data_value)\nPython only. Return a tile column marking as NoData all cells equal to no_data_value.\nThe no_data_value argument is a literal Double, not a Column expression.\nIf input tile had a NoData value already, the behaviour depends on if its cell type is floating point or not. For floating point cell type tile, NoData values on the input tile remain NoData values on the output. For integral cell type tiles, the previous NoData values become literal values.","title":"rf_with_no_data"},{"location":"/reference.html#local-map-algebra","text":"Local map algebra raster operations are element-wise operations on a single tile (unary), between a tile and a scalar, between two tiles, or across many tiles.\nWhen these operations encounter a NoData value in either operand, the cell in the resulting tile will have a NoData.\nThe binary local map algebra functions have similar variations in the Python API depending on the left hand side type:\nrf_local_op: applies op to two columns; the right hand side can be a tile or a numeric column. rf_local_op_double: applies op to a tile and a literal scalar, coercing the tile to a floating point type rf_local_op_int: applies op to a tile and a literal scalar, without coercing the tile to a floating point type\nThe SQL API does not require the rf_local_op_double or rf_local_op_int forms (just rf_local_op).\nLocal map algebra operations for more than two tiles are implemented to work across rows in the DataFrame. As such, they are aggregate functions.","title":"Local Map Algebra"},{"location":"/reference.html#rf-local-add","text":"Tile rf_local_add(Tile tile1, Tile rhs)\nTile rf_local_add_int(Tile tile1, Int rhs)\nTile rf_local_add_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise sum of tile1 and rhs.","title":"rf_local_add"},{"location":"/reference.html#rf-local-subtract","text":"Tile rf_local_subtract(Tile tile1, Tile rhs)\nTile rf_local_subtract_int(Tile tile1, Int rhs)\nTile rf_local_subtract_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise difference of tile1 and rhs.","title":"rf_local_subtract"},{"location":"/reference.html#rf-local-multiply","text":"Tile rf_local_multiply(Tile tile1, Tile rhs)\nTile rf_local_multiply_int(Tile tile1, Int rhs)\nTile rf_local_multiply_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise product of tile1 and rhs. This is not the matrix multiplication of tile1 and rhs.","title":"rf_local_multiply"},{"location":"/reference.html#rf-local-divide","text":"Tile rf_local_divide(Tile tile1, Tile rhs)\nTile rf_local_divide_int(Tile tile1, Int rhs)\nTile rf_local_divide_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise quotient of tile1 and rhs.","title":"rf_local_divide"},{"location":"/reference.html#rf-normalized-difference","text":"Tile rf_normalized_difference(Tile tile1, Tile tile2)\nCompute the normalized difference of the the two tiles: (tile1 - tile2) / (tile1 + tile2). Result is always floating point cell type. This function has no scalar variant.","title":"rf_normalized_difference"},{"location":"/reference.html#rf-local-less","text":"Tile rf_local_less(Tile tile1, Tile rhs)\nTile rf_local_less_int(Tile tile1, Int rhs)\nTile rf_local_less_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise evaluation of tile1 is less than rhs.","title":"rf_local_less"},{"location":"/reference.html#rf-local-less-equal","text":"Tile rf_local_less_equal(Tile tile1, Tile rhs)\nTile rf_local_less_equal_int(Tile tile1, Int rhs)\nTile rf_local_less_equal_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise evaluation of tile1 is less than or equal to rhs.","title":"rf_local_less_equal"},{"location":"/reference.html#rf-local-greater","text":"Tile rf_local_greater(Tile tile1, Tile rhs)\nTile rf_local_greater_int(Tile tile1, Int rhs)\nTile rf_local_greater_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise evaluation of tile1 is greater than rhs.","title":"rf_local_greater"},{"location":"/reference.html#rf-local-greater-equal","text":"Tile rf_local_greater_equal(Tile tile1, Tile rhs)\nTile rf_local_greater_equal_int(Tile tile1, Int rhs)\nTile rf_local_greater_equal_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise evaluation of tile1 is greater than or equal to rhs.","title":"rf_local_greater_equal"},{"location":"/reference.html#rf-local-equal","text":"Tile rf_local_equal(Tile tile1, Tile rhs)\nTile rf_local_equal_int(Tile tile1, Int rhs)\nTile rf_local_equal_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise equality of tile1 and rhs.","title":"rf_local_equal"},{"location":"/reference.html#rf-local-unequal","text":"Tile rf_local_unequal(Tile tile1, Tile rhs)\nTile rf_local_unequal_int(Tile tile1, Int rhs)\nTile rf_local_unequal_double(Tile tile1, Double rhs)\nReturns a tile column containing the element-wise inequality of tile1 and rhs.","title":"rf_local_unequal"},{"location":"/reference.html#rf-round","text":"Tile rf_round(Tile tile)\nRound cell values to the nearest integer without changing the cell type.","title":"rf_round"},{"location":"/reference.html#rf-abs","text":"Tile rf_abs(Tile tile)\nCompute the absolute value of cell value.","title":"rf_abs"},{"location":"/reference.html#rf-exp","text":"Tile rf_exp(Tile tile)\nPerforms cell-wise exponential.","title":"rf_exp"},{"location":"/reference.html#rf-exp10","text":"Tile rf_exp10(Tile tile)\nCompute 10 to the power of cell values.","title":"rf_exp10"},{"location":"/reference.html#rf-exp2","text":"Tile rf_exp2(Tile tile)\nCompute 2 to the power of cell values.","title":"rf_exp2"},{"location":"/reference.html#rf-expm1","text":"Tile rf_expm1(Tile tile)\nPerforms cell-wise exponential, then subtract one. Inverse of log1p.","title":"rf_expm1"},{"location":"/reference.html#rf-log","text":"Tile rf_log(Tile tile)\nPerforms cell-wise natural logarithm.","title":"rf_log"},{"location":"/reference.html#rf-log10","text":"Tile rf_log10(Tile tile)\nPerforms cell-wise logarithm with base 10.","title":"rf_log10"},{"location":"/reference.html#rf-log2","text":"Tile rf_log2(Tile tile)\nPerforms cell-wise logarithm with base 2.","title":"rf_log2"},{"location":"/reference.html#rf-log1p","text":"Tile rf_log1p(Tile tile)\nPerforms natural logarithm of cell values plus one. Inverse of rf_expm1.","title":"rf_log1p"},{"location":"/reference.html#tile-statistics","text":"The following functions compute a statistical summary per row of a tile column. The statistics are computed across the cells of a single tile, within each DataFrame Row.","title":"Tile Statistics"},{"location":"/reference.html#rf-tile-sum","text":"Double rf_tile_sum(Tile tile)\nComputes the sum of cells in each row of column tile, ignoring NoData values.","title":"rf_tile_sum"},{"location":"/reference.html#rf-tile-mean","text":"Double rf_tile_mean(Tile tile)\nComputes the mean of cells in each row of column tile, ignoring NoData values.","title":"rf_tile_mean"},{"location":"/reference.html#rf-tile-min","text":"Double rf_tile_min(Tile tile)\nComputes the min of cells in each row of column tile, ignoring NoData values.","title":"rf_tile_min"},{"location":"/reference.html#rf-tile-max","text":"Double rf_tile_max(Tile tile)\nComputes the max of cells in each row of column tile, ignoring NoData values.","title":"rf_tile_max"},{"location":"/reference.html#rf-no-data-cells","text":"Long rf_no_data_cells(Tile tile)\nReturn the count of NoData cells in the tile.","title":"rf_no_data_cells"},{"location":"/reference.html#rf-data-cells","text":"Long rf_data_cells(Tile tile)\nReturn the count of data cells in the tile.","title":"rf_data_cells"},{"location":"/reference.html#rf-exists","text":"Boolean rf_exists(Tile tile)\nReturns true if any cells in the tile are true (non-zero and not NoData).","title":"rf_exists"},{"location":"/reference.html#rf-for-all","text":"Boolean rf_for_all(Tile tile)\nReturns true if all cells in the tile are true (non-zero and not NoData). See also `rf_is_no_data_tile, which tests that all cells are NoData.","title":"rf_for_all"},{"location":"/reference.html#rf-tile-stats","text":"Struct[Long, Long, Double, Double, Double, Double] rf_tile_stats(Tile tile)\nComputes the following statistics of cells in each row of column tile: data cell count, NoData cell count, minimum, maximum, mean, and variance. The minimum, maximum, mean, and variance are computed ignoring NoData values. Resulting column has the below schema.\nroot\n |-- tile_stats: struct (nullable = true)\n |    |-- data_cells: long (nullable = false)\n |    |-- no_data_cells: long (nullable = false)\n |    |-- min: double (nullable = false)\n |    |-- max: double (nullable = false)\n |    |-- mean: double (nullable = false)\n |    |-- variance: double (nullable = false)","title":"rf_tile_stats"},{"location":"/reference.html#rf-tile-histogram","text":"Struct[Array[Struct[Double, Long]]] rf_tile_histogram(Tile tile)\nComputes a count of cell values within each row of tile. The bins array is of tuples of histogram values and counts. Typically values are plotted on the x-axis and counts on the y-axis. Resulting column has the below schema. Related is the rf_agg_approx_histogram which computes the statistics across all rows in a group.\nroot\n |-- tile_histogram: struct (nullable = true)\n |    |-- bins: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- value: double (nullable = false)\n |    |    |    |-- count: long (nullable = false)","title":"rf_tile_histogram"},{"location":"/reference.html#aggregate-tile-statistics","text":"These functions compute statistical summaries over all of the cell values and across all the rows in the DataFrame or group.","title":"Aggregate Tile Statistics"},{"location":"/reference.html#rf-agg-mean","text":"Double rf_agg_mean(Tile tile)\nSQL: rf_agg_stats(tile).mean\nAggregates over the tile and return the mean of cell values, ignoring NoData. Equivalent to rf_agg_stats.mean.","title":"rf_agg_mean"},{"location":"/reference.html#rf-agg-data-cells","text":"Long rf_agg_data_cells(Tile tile)\nSQL: rf_agg_stats(tile).dataCells\nAggregates over the tile and return the count of data cells. Equivalent to rf_agg_stats.dataCells.","title":"rf_agg_data_cells"},{"location":"/reference.html#rf-agg-no-data-cells","text":"Long rf_agg_no_data_cells(Tile tile)\nSQL: rf_agg_stats(tile).dataCells\nAggregates over the tile and return the count of NoData cells. Equivalent to rf_agg_stats.noDataCells. C.F. rf_no_data_cells a row-wise count of no data cells.","title":"rf_agg_no_data_cells"},{"location":"/reference.html#rf-agg-stats","text":"Struct[Long, Long, Double, Double, Double, Double] rf_agg_stats(Tile tile)\nAggregates over the tile and returns statistical summaries of cell values: number of data cells, number of NoData cells, minimum, maximum, mean, and variance. The minimum, maximum, mean, and variance ignore the presence of NoData.","title":"rf_agg_stats"},{"location":"/reference.html#rf-agg-approx-histogram","text":"Struct[Array[Struct[Double, Long]]] rf_agg_approx_histogram(Tile tile)\nAggregates over all of the rows in DataFrame of tile and returns a count of each cell value to create a histogram with values are plotted on the x-axis and counts on the y-axis. Related is the rf_tile_histogram function which operates on a single row at a time.","title":"rf_agg_approx_histogram"},{"location":"/reference.html#tile-local-aggregate-statistics","text":"Local statistics compute the element-wise statistics across a DataFrame or group of tiles, resulting in a tile that has the same dimension.\nWhen these functions encounter NoData in a cell location, it will be ignored.","title":"Tile Local Aggregate Statistics"},{"location":"/reference.html#rf-agg-local-max","text":"Tile rf_agg_local_max(Tile tile)\nCompute the cell-local maximum operation over tiles in a column.","title":"rf_agg_local_max"},{"location":"/reference.html#rf-agg-local-min","text":"Tile rf_agg_local_min(Tile tile)\nCompute the cell-local minimum operation over tiles in a column.","title":"rf_agg_local_min"},{"location":"/reference.html#rf-agg-local-mean","text":"Tile rf_agg_local_mean(Tile tile)\nCompute the cell-local mean operation over tiles in a column.","title":"rf_agg_local_mean"},{"location":"/reference.html#rf-agg-local-data-cells","text":"Tile rf_agg_local_data_cells(Tile tile)\nCompute the cell-local count of data cells over tiles in a column. Returned tile has a cell type of int32.","title":"rf_agg_local_data_cells"},{"location":"/reference.html#rf-agg-local-no-data-cells","text":"Tile rf_agg_local_no_data_cells(Tile tile)\nCompute the cell-local count of NoData cells over tiles in a column. Returned tile has a cell type of int32.","title":"rf_agg_local_no_data_cells"},{"location":"/reference.html#rf-agg-local-stats","text":"Struct[Tile, Tile, Tile, Tile, Tile] rf_agg_local_stats(Tile tile)\nCompute cell-local aggregate count, minimum, maximum, mean, and variance for a column of tiles. Returns a struct of five tiles.","title":"rf_agg_local_stats"},{"location":"/reference.html#converting-tiles","text":"RasterFrames provides several ways to convert a tile into other data structures. See also functions for creating tiles.","title":"Converting Tiles"},{"location":"/reference.html#rf-explode-tiles","text":"Int, Int, Numeric* rf_explode_tiles(Tile* tile)\nCreate a row for each cell in tile columns. Many tile columns can be passed in, and the returned DataFrame will have one numeric column per input. There will also be columns for column_index and row_index. Inverse of rf_assemble_tile. When using this function, be sure to have a unique identifier for rows in order to successfully invert the operation.","title":"rf_explode_tiles"},{"location":"/reference.html#rf-explode-tiles-sample","text":"Int, Int, Numeric* rf_explode_tiles_sample(Double sample_frac, Long seed, Tile* tile)\nPython only. As with rf_explode_tiles, but taking a randomly sampled subset of cells. Equivalent to the rf_explode-tiles, but allows a random subset of the data to be selected. Parameter sample_frac should be between 0.0 and 1.0.","title":"rf_explode_tiles_sample"},{"location":"/reference.html#rf-tile-to-array-int","text":"Array rf_tile_to_array_int(Tile tile)\nConvert Tile column to Spark SQL Array, in row-major order. Float cell types will be coerced to integral type by flooring.","title":"rf_tile_to_array_int"},{"location":"/reference.html#rf-tile-to-array-double","text":"Array rf_tile_to_arry_double(Tile tile)\nConvert tile column to Spark Array, in row-major order. Integral cell types will be coerced to floats.","title":"rf_tile_to_array_double"},{"location":"/reference.html#rf-render-ascii","text":"String rf_render_ascii(Tile tile)\nPretty print the tile values as plain text.","title":"rf_render_ascii"},{"location":"/reference.html#rf-render-matrix","text":"String rf_render_matrix(Tile tile)\nRender Tile cell values as numeric values, for debugging purposes.","title":"rf_render_matrix"},{"location":"/reference.html#rf-rgb-composite","text":"Tile rf_rgb_composite(Tile red, Tile green, Tile blue)\nMerges three bands into a single byte-packed RGB composite. It first scales each cell to fit into an unsigned byte, in the range 0-255, and then merges all three channels to fit into a 32-bit unsigned integer. This is useful when you want an RGB tile to render or to process with other color imagery tools.","title":"rf_rgb_composite"},{"location":"/reference.html#rf-render-png","text":"Array rf_render_png(Tile red, Tile green, Tile blue)\nRuns rf_rgb_composite on the given tile columns and then encodes the result as a PNG byte array.","title":"rf_render_png"},{"location":"/release-notes.html","text":"","title":"Release Notes"},{"location":"/release-notes.html#release-nbsp-notes","text":"","title":"Release Notes"},{"location":"/release-notes.html#0-8-x","text":"","title":"0.8.x"},{"location":"/release-notes.html#0-8-1","text":"Added rf_local_no_data, rf_local_data and rf_interpret_cell_type_as raster functions. Added: rf_rgb_composite and rf_render_png. Added toMarkdown and toHTML extension methods for DataFrame, and registered them with the IPython formatter system when rf_ipython is imported. New documentation theme (thanks @jonas!). Fixed: Removed false return type guarantee in cases where an Expression accepts either Tile or ProjectedRasterTile (#295)","title":"0.8.1"},{"location":"/release-notes.html#0-8-0","text":"Super-duper new Python-centric RasterFrames Users’ Manual! Upgraded to the following core dependencies: Spark 2.3.3, GeoTrellis 2.3.0, GeoMesa 2.2.1, JTS 1.16.0. Build pyrasterframes binary distribution for pip installation. Added support for rendering RasterFrame types in IPython/Jupyter. Added new tile functions rf_round, rf_abs, rf_log, rf_log10, rf_log2, rf_log1p, rf_exp, rf_exp10, rf_exp2, rf_expm1, rf_resample. Support Python-side Tile User-Defined Type backed by numpy ndarray or ma.MaskedArray. Support Python-side Shapely geometry User-Defined Type. SQL API support for rf_assemble_tile and rf_array_to_tile. Introduced at the source level the concept of a RasterSource and RasterRef, enabling lazy/delayed read of sub-scene tiles. Added withKryoSerialization extension methods on SparkSession.Builder and SparkConf. Added rf_render_matrix debugging function. Added RasterFrameLayer.withExtent extension method. Added SinglebandGeoTiff.toDF extension method. Added DataFrame.rasterJoin extension method for merging two dataframes with tiles in disparate CRSs. Added rf_crs for ProjectedRasterTile columns. Added st_extent (for Geometry types) and rf_extent (for ProjectedRasterTile and RasterSource columns). Added st_geometry (for Extent types) and rf_geometry (for ProjectedRasterTile and RasterSource columns). Reworked build scripts for RasterFrames Jupyter Notebook. Breaking: The type RasterFrame renamed RasterFrameLayer to be reflect its intended purpose. Breaking: All asRF methods renamed to asLayer. Breaking: Root package changed from org.locationtech.rasterframes to org.locationtech.rasterframes. Breaking: Removed envelope, in lieu of st_extent, rf_extent or st_envelope Breaking: Renamed rf_extent_geometry to st_geometry Breaking: Renamed rf_tile_dimensions to rf_dimensions Breaking: Renamed rf_reproject_geometry to st_reproject Breaking: With the upgrade to JTS 1.16.0, all imports of com.vividsolutions.jts need to be changed to org.locationtech.jts. Deprecation: Tile column functions (in RasterFunctions) and SQL registered names have all been renamed to follow snake_case conventions, with an rf_ prefix, matching SQL and Python. A temporary compatibility shim is included so that code built against 0.7.1 and earlier still work. These will be marked as deprecated. Breaking: In Scala and SQL, ..._scalar functions (e.g. local_add_scalar) have been removed. Non-scalar forms now dynamically detect type of right hand side. Breaking: tileToArray has been replaced with _tile_to_array_double and _tile_to_array_int. Breaking: Renamed bounds_geometry to rf_extent_geometry. Breaking: renamed agg_histogram to rf_agg_approx_histogram, local_agg_stats to rf_agg_local_stats, local_agg_max to rf_agg_local_max, local_agg_min to rf_agg_local_min, local_agg_mean to rf_agg_local_mean, local_agg_data_cells to rf_agg_local_data_cells, local_agg_no_data_cells to rf_agg_local_no_data_cells. Breaking: CellHistogram no longer carries along approximate statistics, due to confusing behavior. Use rf_agg_stats instead. Introduced LocalCellStatistics class to wrap together results from LocalStatsAggregate. Breaking: TileDimensions moved from astraea.spark.rasterframes to org.locationtech.rasterframes.model. Breaking: Renamed RasterFrame.withBounds to RasterFrameLayer.withGeometry for consistency with DataSource schemas.","title":"0.8.0"},{"location":"/release-notes.html#known-issues","text":"#188: Error on deserialization of a Tile with a bool cell type to the Python side; see issue description for work around.","title":"Known issues"},{"location":"/release-notes.html#0-7-x","text":"","title":"0.7.x"},{"location":"/release-notes.html#0-7-1","text":"Fixed ColorRamp pipeline in MultibandRender Fixed Python wrapper for explodeTiles","title":"0.7.1"},{"location":"/release-notes.html#0-7-0","text":"Now an incubating project under Eclipse Foundation LocationTech! GitHub repo moved to locationtech/rasterframes. PySpark support! See pyrasterframes/python/README.rst to get started. Exposed Spark JTS spatial operations in Python. Added RasterFrames-enabled Jupyter Notebook Docker Container package. See deployment/README.md for details. Updated to GeoMesa version 2.0.1. Added convertCellType, normalizedDifference mask and inverseMask operations on tile columns. Added tile column + scalar operations: localAddScalar, localSubtractScalar, localMultiplyScalar, localDivideScalar Added rasterize and reprojectGeometry operations on geometry columns. Added for for writing GeoTIFFs from RasterFrames via DataFrameWriter. Added spark.read.geotrellis.withNumPartitions(Int) for setting the initial number of partitions to use when reading a layer. Added spark.read.geotrellis.withTileSubdivisions(Int) for evenly subdividing tiles before they become rows in a RasterFrame. Added experimental package for sandboxing new feature ideas. Added experimental GeoJSON DataSource with schema inferfence on feature properties. Added Scala, SQL, and Python tile-scalar arithmetic operations: localAddScalar, localSubtractScalar, localMultipyScalar, localDivideScalar. Added Scala, SQL, and Python tile functions for logical comparisons both tile-tile and tile-scalar variants: localLess, localLessEqual, localGreater, localGreaterEqual, localEqual, and localUnequal. Added SlippyExport experimental feature for exporting the contents of a RasterFrame as a SlippyMap tile image directory structure and Leaflet/OpenMaps-enabled HTML file. Added experimental DataSource implementations for MODIS and Landsat 8 catalogs on AWS PDS. Change: Default interpoation for toRaster and toMultibandRaster has been changed from Bilinear to NearestNeighbor. Breaking: Renamed/moved astraea.spark.rasterframes.functions.CellStatsAggregateFunction.Statistics to astraea.spark.rasterframes.stats.CellStatistics. Breaking: HistogramAggregateFunction now generates the new type astraea.spark.rasterframes.stats.CellHistogram. Breaking: box2D renamed envelope.","title":"0.7.0"},{"location":"/release-notes.html#0-6-x","text":"","title":"0.6.x"},{"location":"/release-notes.html#0-6-1","text":"Added support for reading striped GeoTiffs (#64). Moved extension methods associated with querying tagged columns to DataFrameMethods for supporting temporal and spatial columns on non-RasterFrame DataFrames. GeoTIFF and GeoTrellis DataSources automatically initialize RasterFrames. Added RasterFrame.toMultibandRaster. Added utility for rendering multiband tile as RGB composite PNG. Added RasterFrame.withRFColumnRenamed to lessen boilerplate in maintaining RasterFrame type tag.","title":"0.6.1"},{"location":"/release-notes.html#0-6-0","text":"Upgraded to Spark 2.2.1. Added VersionShims to allow for Spark 2.1.x backwards compatibility. Introduced separate rasterframes-datasource library for hosting sources from which to read RasterFrames. Implemented basic (but sufficient) temporal and spatial filter predicate push-down feature for the GeoTrellis layer datasource. Added Catalyst expressions specifically for spatial relations, allowing for some polymorphism over JTS types. Added a GeoTrellis Catalog DataSource for inspecting available layers and associated metadata at a URI Added GeoTrellis Layer DataSource for reading GeoTrellis layers from any SPI-registered GeoTrellis backend (which includes HDFS, S3, Accumulo, HBase, Cassandra, etc.). Ability to save a RasterFrame as a GeoTrellis layer to any SPI-registered GeoTrellis backends. Multi-column RasterFrames are written as Multiband tiles. Addd a GeoTiff DataSource for directly loading a (preferably Cloud Optimized) GeoTiff as a RasterFrame, each row containing tiles as they are internally organized. Fleshed out support for MultibandTile and TileFeature support in datasource. Added typeclass for specifying merge operations on TileFeature data payload. Added withTemporalComponent convenince method for creating appending a temporal key column with constant value. Breaking: Renamed withExtent to withBounds, and now returns a JTS Polygon. Added EnvelopeEncoder for encoding JTS Envelope type. Refactored build into separate core and docs, paving way for pyrasterframes polyglot module. Added utility extension method withPrefixedColumnNames to DataFrame.","title":"0.6.0"},{"location":"/release-notes.html#known-issues","text":"Writing multi-column RasterFrames to GeoTrellis layers requires all tiles to be of the same cell type.","title":"Known Issues"},{"location":"/release-notes.html#0-5-x","text":"","title":"0.5.x"},{"location":"/release-notes.html#0-5-12","text":"Added withSpatialIndex to introduce a column assigning a z-curve index value based on the tile’s centroid in EPSG:4326. Added column-appending convenience methods: withExtent, withCenter, withCenterLatLng Documented example of creating a GeoTrellis layer from a RasterFrame. Added Spark 2.2.0 forward-compatibility. Upgraded to GeoTrellis 1.2.0-RC2.","title":"0.5.12"},{"location":"/release-notes.html#0-5-11","text":"Significant performance improvement in explodeTiles (1-2 orders of magnitude). See #38 Fixed bugs in NoData handling when converting to Double tiles.","title":"0.5.11"},{"location":"/release-notes.html#0-5-10","text":"Upgraded to shapeless 2.3.2 Fixed #36, #37","title":"0.5.10"},{"location":"/release-notes.html#0-5-9","text":"Ported to sbt 1.0.3 Added sbt-generated astraea.spark.rasterframes.RFBuildInfo Fixed bug in computing aggMean when one or more tiles are null Deprecated rfIinit in favor of SparkSession.withRasterFrames or SQLContext.withRasterFrames extension methods","title":"0.5.9"},{"location":"/release-notes.html#0-5-8","text":"Upgraded to GeoTrellis 1.2.0-RC1 Added REPLsent-based tour of RasterFrames Moved Giter8 template to separate repository s22s/raster-frames.g8 due to sbt limitations Updated Getting Started to reference new Giter8 repo Changed SQL function name rf_stats and rf_histogram to rf_aggStats and rf_aggHistogram for consistency with DataFrames API","title":"0.5.8"},{"location":"/release-notes.html#0-5-7","text":"Created faster implementation of aggregate statistics. Fixed bug in deserialization of TileUDTs originating from ConstantTiles Fixed bug in serialization of NoDataFilter within SparkML pipeline Refactoring of UDF organization Various documentation tweaks and updates Added Giter8 template","title":"0.5.7"},{"location":"/release-notes.html#0-5-6","text":"TileUDFs are encoded using directly into Catalyst–without Kryo–resulting in an insane decrease in serialization time for small tiles (int8, <= 128²), and pretty awesome speedup for all other cell types other than float32 (marginal slowing). While not measured, memory footprint is expected to have gone down.","title":"0.5.6"},{"location":"/release-notes.html#0-5-5","text":"aggStats and tileMean functions rewritten to compute simple statistics directly rather than using StreamingHistogram tileHistogramDouble and tileStatsDouble were replaced by tileHistogram and tileStats Added tileSum, tileMin and tileMax functions Added aggMean, aggDataCells and aggNoDataCells aggregate functions. Added localAggDataCells and localAggNoDataCells cell-local (tile generating) fuctions Added tileToArray and arrayToTile Overflow fix in LocalStatsAggregateFunction","title":"0.5.5"}]}