# Writing Raster Data

RasterFrames is oriented toward large scale analyses of spatial data. The primary output for most use cases may be a @ref:[statistical summary](aggregation.md), a @ref:[machine learning model](machine-learning.md), or some other result that is generally much smaller than the input data set. 

However there are times in any analysis where writing a representative sample of the work in progress provides invaluable feedback on the process and results.

```python imports, echo=False
import pyrasterframes
from pyrasterframes.rasterfunctions import *
from IPython.display import display
spark = pyrasterframes.get_spark_session()
```

## Tile Samples

When collecting a _tile_ (see discussion of the RasterFrame @ref:[schema](raster-read.md#single-raster) for orientation to the concept) to the Python Spark driver, we have some convenience methods to quickly visualize the _tile_. 

In an IPython or Jupyter interpreter a `Tile` object will be displayed as an image with limited metadata.

```python tile_sample
def scene(band): 
    return 'https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/' \
             'MCD43A4.A2019059.h11v08.006.2019072203257_B{}.TIF'.format(band)
raster_url = scene('02')
spark_df = spark.read.raster(raster_url)
tile = spark_df.select(rf_tile('proj_raster').alias('tile')).first()['tile']
tile
```

```python display_tile, echo=False, output=True
display(tile) # IPython.display function
```

## DataFrame Samples

Within an IPython or Jupyter interpreter a Pandas DataFrame containing a column of _tiles_ will be rendered as the samples discussed above. Simply import the `rf_ipython` submodule to enable enhanced HTML rendering of a Pandas DataFrame.

In the example below, notice the result is limited to a small subset. For more discussion about why this is important, see the @ref:[Pandas and NumPy discussion](numpy-pandas.md).

```python to_pandas, evaluate=True
import pyrasterframes.rf_ipython

pandas_df = spark.read.raster(raster_url, tile_dimensions=(64, 64)) \
                .select(
                    rf_extent('proj_raster').alias('extent'),
                    rf_tile('proj_raster').alias('tile'),
                ).limit(5).toPandas()
pandas_df.dtypes
```

Viewing the DataFrame in Jupyter looks like this. 

```python show_pandas, evaluate=False
pandas_df
```

@@include[df-samples-output.md](static/df-samples-output.md)


## GeoTIFFs

GeoTIFF is one of the most common file formats for spatial data, providing flexibility in data encoding, representation, and storage. RasterFrames provides a specialized Spark DataFrame writer for rendering a RasterFrame to a GeoTiff.

One downside to GeoTIFF is that it is not a big data native format. To create a GeoTIFF all the data to be encoded has to be in the memory of one compute node (in Spark parlance, this is a "collect"), limiting it's maximum size substantially compared to that of a full cluster environment. When rendering GeoTIFFs in RasterFrames, you either need to specify the dimensions of the output raster, or be aware of how big the collected data will end up being.

Fortunately, we can use the cluster computing capability to downlample the data into a more manageble saze. For sake of example, let's render a simple RGB overview image of our scene as a small raster:

```python write_geotiff
import os.path
from docs import docs_dir
cat = """  
red,green,blue
{},{},{}
""".format(scene('01'), scene('04'), scene('03'))
outfile = os.path.join(docs_dir(), 'geotiff-overview.tif')
rf = spark.read.raster(catalog=cat, catalog_col_names=['red', 'green', 'blue'])
rf.write.geotiff(outfile, crs='EPSG:4326', raster_dimensions=(256, 256))
```

Wiew it with `rasterio` to check the results:

```python view_geotiff
import rasterio
import numpy as np
from rasterio.plot import show, show_hist
 
with rasterio.open(outfile) as src:
    cells = np.clip(src.read(), 0, 1800).astype('float32')
    show(cells)
    show_hist(src, bins=50, lw=0.0, stacked=False, alpha=0.3,
        histtype='stepfilled', title="Overview Histogram")
```


## GeoTrellis Layers

[GeoTrellis][GeoTrellis] is one of the key libraries that RasterFrames builds upon. It provides a Scala language API to working with large raster data with Apache Spark. Ingesting raster data into a Layer is one of the key concepts for creating a dataset for processing on Spark. RasterFrames write data from an appropriate DataFrame into a GeoTrellis Layer.

\[ More details see https://s22s.myjetbrains.com/youtrack/issue/RF-72 \]


## Parquet

You can write the Spark DataFrame to an [Apache Parquet][Parquet] "file". This format is designed to work across different projects in the Hadoop ecosystem. It also provides a variety of optimizations for query against data written in the format. 

```python write_parquet, evaluate=False
spark_df.withColumn('exp', rf_expm1('proj_raster')) \
    .write.mode('append').parquet('hdfs:///rf-user/sample.pq')
```

[GeoTrellis]: https://geotrellis.readthedocs.io/en/latest/
[Parquet]: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html